{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Q&A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts.tf2_0_baseline_w_bert_translated_to_tf2_0 import AnswerType\n",
    "from scripts import albert\n",
    "from scripts import albert_tokenization\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"albert\",\n",
    "    \"The name of model to use. Choose from ['bert', 'albert'].\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the ALBERT/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"data/train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", 272565,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_checkpoint_file\", \"tf2_albert_finetuned.ckpt\",\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"models/albert_xl/tf2_model.h5\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# This should be changed to 512 at some point,\n",
    "# as training was done with that value, it may\n",
    "# not make a big difference though\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"data/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                            \"non-floating point (and non-complex) \"\n",
    "                            \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                             \"`TDense` should be defined. \"\n",
    "                             \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30209\n",
    "\n",
    "# this is the helper function to create the albert model\n",
    "# config_file is used to create the model\n",
    "# pretrain_ckpt is used to load the pretrain weights except for the embedding layer\n",
    "def get_albert_model(config_file, max_seq_length, vocab_size, pretrain_ckpt=None):\n",
    "    \"\"\" create albert model from pretrained configuration file with vocab_size changed to VOCAB_SIZE\n",
    "        and optionally loads the pretrained weights\n",
    "    \"\"\"\n",
    "    \n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    config.vocab_size = vocab_size\n",
    "    albert_layer = albert.AlbertModel(config=config)\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "    pooled_output, sequence_output = albert_layer(input_word_ids=input_ids,\n",
    "                                                    input_mask=input_mask,\n",
    "                                                    input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    albert_model = tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='albert')\n",
    "    \n",
    "    if pretrain_ckpt:\n",
    "        load_pretrain_weights(albert_model, config_file, pretrain_ckpt, max_seq_length)\n",
    "        \n",
    "    return albert_model\n",
    "\n",
    "def load_pretrain_weights(model, config_file, ckpt_file, max_seq_length):\n",
    "    \"\"\"loads the pretrained model's weights, except for the embedding layer,\n",
    "    into the new model, which has [0:29999] loaded\n",
    "    \n",
    "    Args:\n",
    "        model: the same model architecture as the pre-trained model except for embedding\n",
    "        config_file: path to the config file to re-create the pre-trained model\n",
    "        ckpt_file: path to the checkpoint of the pre-trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    # re-create the pre-trained model\n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    albert_layer_pretrain = albert.AlbertModel(config=config, name='albert_pretrain')\n",
    "\n",
    "    input_ids_pretrain = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='input_ids_pretrain')\n",
    "    input_mask_pretrain = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='input_mask_pretrain')\n",
    "    segment_ids_pretrain = tf.keras.Input(shape=(max_seq_length,),dtype=tf.int32,name='segment_ids_pretrain')\n",
    "\n",
    "    pooled_output_pretrain, sequence_output_pretrain = albert_layer_pretrain(input_word_ids=input_ids_pretrain,\n",
    "                                                    input_mask=input_mask_pretrain,\n",
    "                                                    input_type_ids=segment_ids_pretrain)\n",
    "\n",
    "    albert_model_pretrain = tf.keras.Model(inputs=[input_ids_pretrain,input_mask_pretrain,segment_ids_pretrain], \n",
    "           outputs=[pooled_output_pretrain, sequence_output_pretrain])\n",
    "    \n",
    "    # load the weights into the pre-trained model\n",
    "    albert_model_pretrain.load_weights(ckpt_file)\n",
    "    \n",
    "    # set the pre-train weights on the new model\n",
    "    albert_layer = model.get_layer('albert_model')\n",
    "    albert_layer.embedding_postprocessor.set_weights(albert_layer_pretrain.embedding_postprocessor.get_weights())\n",
    "    albert_layer.encoder.set_weights(albert_layer_pretrain.encoder.get_weights())\n",
    "    albert_layer.pooler_transform.set_weights(albert_layer_pretrain.pooler_transform.get_weights())\n",
    "    # load the embedding\n",
    "    embedding_weights = albert_layer.embedding_lookup.get_weights()\n",
    "    embedding_weights_pretrain = albert_layer_pretrain.embedding_lookup.get_weights()\n",
    "    # the embedding weights are stored in a list of size 1, so we need to do [0] to get the actual weights\n",
    "    new_embedding_weights = tf.concat([embedding_weights_pretrain[0], embedding_weights[0][30000:]],axis=0)\n",
    "    # then we unsqueeze the first dimension after concat\n",
    "    new_embedding_weights = tf.expand_dims(new_embedding_weights, axis=0)\n",
    "    albert_layer.embedding_lookup.set_weights(new_embedding_weights)\n",
    "\n",
    "# function that builds bert/albert from config, optionally loads the pretrain weights for albert\n",
    "def build_model(config_file, max_seq_length, init_ckpt, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\" build model according to model_name\n",
    "    \n",
    "    Args:\n",
    "        model_name: ['bert', 'albert']\n",
    "        config_file: path to config file\n",
    "        max_seq_length: the maximum length for each scan\n",
    "        pretrain_ckpt: path to pretrain checkpoint (albert only)\n",
    "        vocab_size: size of the new vocab, (albert only)\n",
    "    Returns:\n",
    "        the specified model\n",
    "    \"\"\"\n",
    "\n",
    "    model = get_albert_model(config_file=config_file, \n",
    "                             max_seq_length=max_seq_length, \n",
    "                             pretrain_ckpt=init_ckpt,\n",
    "                             vocab_size=vocab_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config_file=FLAGS.config_file,\n",
    "                    max_seq_length=FLAGS.max_seq_length,\n",
    "                    init_ckpt=FLAGS.init_checkpoint)\n",
    "\n",
    "model.save_weights('new_albert_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
