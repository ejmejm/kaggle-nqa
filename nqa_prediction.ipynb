{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ&A Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts.tf2_0_baseline_w_bert_translated_to_tf2_0 import AnswerType\n",
    "from scripts import bert_modeling as modeling\n",
    "from scripts import bert_tokenization\n",
    "from scripts import albert\n",
    "from scripts import albert_tokenization\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partial_nq_examples(input_file, is_training, n=-1):\n",
    "    \"\"\"Read a NQ json file into a list of NqExample.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_file)\n",
    "    input_data = []\n",
    "\n",
    "    def _open(path):\n",
    "        if path.endswith(\".gz\"):\n",
    "            return gzip.GzipFile(fileobj=tf.io.gfile.GFile(path, \"rb\"))\n",
    "        else:\n",
    "            return tf.io.gfile.GFile(path, \"r\")\n",
    "\n",
    "    for path in input_paths:\n",
    "        absl.logging.info(\"Reading: %s\", path)\n",
    "        with _open(path) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                input_data.append(tf2baseline.create_example_from_jsonl(line))\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        examples.extend(tf2baseline.read_nq_entry(entry, is_training))\n",
    "    return examples\n",
    "\n",
    "tf2baseline.read_nq_examples = read_partial_nq_examples\n",
    "\n",
    "def read_partial_candidates_from_one_split(input_path, n=-1):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                        \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                \n",
    "    return candidates_dict\n",
    "\n",
    "tf2baseline.read_candidates_from_one_split = read_partial_candidates_from_one_split\n",
    "\n",
    "def read_partial_candidates(input_pattern, n=-1):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for i, input_path in enumerate(input_paths):\n",
    "        final_dict.update(tf2baseline.read_candidates_from_one_split(input_path, n=n-len(final_dict.keys())))\n",
    "        if len(final_dict.keys()) >= n:\n",
    "                break\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "tf2baseline.read_candidates = read_partial_candidates\n",
    "\n",
    "def raw_data_generator(path, chunk_size=1000):\n",
    "        \"\"\"Reads raw JSON examples to a DataFrame\"\"\"\n",
    "        curr_pos = 0\n",
    "        last_line = False\n",
    "        with open(path, 'rt') as f:\n",
    "                while not last_line:\n",
    "                        df = []\n",
    "                        for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                                line = f.readline()\n",
    "                                if line is None:\n",
    "                                        last_line = True\n",
    "                                        break\n",
    "                                df.append(json.loads(line))\n",
    "                        curr_pos = i + 1\n",
    "                        yield pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"albert\",\n",
    "    \"The name of model to use. Choose from ['bert', 'albert'].\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xxl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the ALBERT/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"models/albert_xxl/albert_finetuned.h5\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 1,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", 1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "# TODO(Edan): Look at nested contents too at some point\n",
    "# Around 5% of long answers are nested, and around 50% of questions have\n",
    "# long answers\n",
    "# This means that this setting alone restricts us from a correct answer\n",
    "# around 2.5% of the time\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", 200,\n",
    "    \"Number of examples to read from files.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"test_post_processing\", True,\n",
    "    \"If true, training data will be predicted for instead of eval data,\"\n",
    "    \"and the predictions will be used to tune the post processing algorithm.\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                          \"non-floating point (and non-complex) \"\n",
    "                          \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                           \"`TDense` should be defined. \"\n",
    "                           \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
    "\n",
    "def get_bert_model(config_file):\n",
    "    \"\"\"Builds and returns a BERT model.\"\"\"\n",
    "    config = modeling.BertConfig.from_json_file(config_file)\n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    bert_layer = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n",
    "                                                input_mask=input_mask,\n",
    "                                                input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    return tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='bert_baseline')\n",
    "\n",
    "def get_albert_model(config_file):\n",
    "    \"\"\"Create an Albert model from pretrained configuration file with vocab_size changed\n",
    "    to 30522, and optionally loads the pretrained weights.\n",
    "    \"\"\"\n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    config.vocab_size = 30209\n",
    "    albert_layer = albert.AlbertModel(config=config)\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "    pooled_output, sequence_output = albert_layer(input_word_ids=input_ids,\n",
    "                                                    input_mask=input_mask,\n",
    "                                                    input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    albert_model = tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='albert')\n",
    "        \n",
    "    return albert_model\n",
    "\n",
    "def build_model(model_name, config_file):\n",
    "    \"\"\"Build model according to model_name.\n",
    "    \n",
    "    Args:\n",
    "        model_name: ['bert', 'albert']\n",
    "        config_file: path to config file\n",
    "        pretrain_ckpt: path to pretrain checkpoint (albert only)\n",
    "    Returns:\n",
    "        the specified model\n",
    "    \"\"\"\n",
    "    if model_name == 'albert':\n",
    "        model = get_albert_model(config_file)\n",
    "    elif model_name == 'bert':\n",
    "        model = get_bert_model(config_file)\n",
    "    else:\n",
    "        raise ValueError('{} is not supported'.format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"albert\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "albert_model_1 (AlbertModel)    ((None, 4096), (None 222622336   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "td_seq (TDense)                 (None, 384, 2)       8194        albert_model_1[0][1]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 384, 1), (No 0           td_seq[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_logits_1 (Ten [(None, 384)]        0           tf_op_layer_split_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_logits_1 (Tenso [(None, 384)]        0           tf_op_layer_split_1[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "ans_type_logits (TDense)        (None, 5)            20485       albert_model_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 222,651,015\n",
      "Trainable params: 222,651,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(FLAGS.model, FLAGS.config_file)\n",
    "model.load_weights(FLAGS.init_checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Predict File: data/simplified-nq-train.jsonl\n",
      "**Features**\n",
      "\n",
      "answer_text\n",
      "answer_type\n",
      "doc_span_index\n",
      "end_position\n",
      "example_index\n",
      "input_ids\n",
      "input_mask\n",
      "segment_ids\n",
      "start_position\n",
      "token_is_max_context\n",
      "token_to_orig_map\n",
      "tokens\n",
      "unique_id\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.do_predict:\n",
    "    if FLAGS.model == 'bert':\n",
    "        tokenizer = bert_tokenization.FullTokenizer(\n",
    "            vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    elif FLAGS.model == 'albert':\n",
    "        tokenizer = albert_tokenization.FullTokenizer(\n",
    "            None, spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "    if FLAGS.test_post_processing:\n",
    "        input_file = FLAGS.train_file\n",
    "    else:\n",
    "        input_file = FLAGS.predict_file\n",
    "\n",
    "    # This is actually quite slow, but I'm not sure if it's due to tokenizing or reading from a compressed file\n",
    "    eval_examples = tf2baseline.read_nq_examples(\n",
    "          input_file=input_file, is_training=FLAGS.test_post_processing, n=FLAGS.n_examples)\n",
    "\n",
    "    print(len(eval_examples))\n",
    "\n",
    "    print(\"Predict File:\", input_file)\n",
    "\n",
    "    eval_writer = tf2baseline.FeatureWriter(\n",
    "      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "      is_training=FLAGS.test_post_processing)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        eval_features.append(feature)\n",
    "        eval_writer.process_feature(feature)\n",
    "\n",
    "    num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "      examples=eval_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      is_training=FLAGS.test_post_processing,\n",
    "      output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "    eval_filename = eval_writer.filename\n",
    "\n",
    "    print('**Features**\\n')\n",
    "\n",
    "    for e in dir(eval_features[0]):\n",
    "        if not e.startswith('__'):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **eval_features**\n",
    "- `doc_span_index`: Which index this span is in the set that makes up one question-article pair\n",
    "- `example_index`: Index of the example \n",
    "- `input_mask`: 0 if a token is padding, otherwise 1 for each token in the article\n",
    "- `segment_ids`: Tokens that are part of a question are 0 ([CLS], [Q], ..., [SEP])\n",
    "- `inpud_ids`: Token ids of the article\n",
    "    - 0 -> [PAD]\n",
    "    - 101 -> [CLS]\n",
    "    - 102 -> [SEP]\n",
    "    - 103 -> [MASK]\n",
    "    - 104 -> [Q]\n",
    "    - 105 -> [YES]\n",
    "    - 106 -> [NO]\n",
    "    - 107 -> [NoLongAnswer]\n",
    "    - 108 -> [NoShortAnswer]\n",
    "- `token_is_max_context`: False if this token will appear again in the next document span due to a sliding window being used, and True otherwise\n",
    "- `token_to_orig_map`: Maps the tokens in `tokens` and `input_ids` to the actual token indices in the original document\n",
    "- `tokens`: A list of tokens making up the document span\n",
    "- `unique_id`: A unique id number for this article (NOT this document span) \n",
    "\n",
    "*Note - mappings are 0 indexed and exlude tokens that are part of the question (those can be identified from segment ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def data_generator(batch_size=32, n_samples=-1, seed=42):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "#     if FLAGS.test_post_processing:\n",
    "#         eval_filename = FLAGS.train_precomputed_file\n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(eval_filename)\n",
    "    if FLAGS.do_train:\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=5000, seed=seed)\n",
    "        \n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    \n",
    "    data_iter = iter(dataset)\n",
    "    sample_idx = 0\n",
    "    while sample_idx < n_samples or n_samples == -1:\n",
    "        try:\n",
    "            examples = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        cutoff_amt = batch_size\n",
    "        if n_samples > -1:\n",
    "            cutoff_amt = min(cutoff_amt, n_samples - sample_idx)\n",
    "        sample_idx += cutoff_amt\n",
    "    \n",
    "        for k, v in examples.items():\n",
    "            examples[k] = v[:cutoff_amt]\n",
    "        \n",
    "        inputs = {\n",
    "            # 'unique_id': examples['unique_ids'],\n",
    "            'input_ids': examples['input_ids'],\n",
    "            'input_mask': examples['input_mask'],\n",
    "            'segment_ids': examples['segment_ids']\n",
    "        }\n",
    "\n",
    "        if FLAGS.do_train:\n",
    "            targets = {\n",
    "                'tf_op_layer_start_logits': examples['start_positions'],\n",
    "                'tf_op_layer_end_logits': examples['end_positions'],\n",
    "                'ans_type_logits': examples['answer_types'],\n",
    "            }\n",
    "\n",
    "            yield inputs, targets\n",
    "        else:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = np.ceil(eval_writer.num_features / FLAGS.predict_batch_size)\n",
    "generator = data_generator(FLAGS.predict_batch_size)\n",
    "\n",
    "preds = model.predict_generator(generator, steps=n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Types\n",
    "- UNKNOWN = 0\n",
    "- YES = 1\n",
    "- NO = 2\n",
    "- SHORT = 3\n",
    "- LONG = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Computing Answers from Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_probs(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "def get_candidate_span(entry):\n",
    "    return (entry['candidates'][entry['candidate_idx']]['start_token'],\n",
    "            entry['candidates'][entry['candidate_idx']]['end_token'])\n",
    "\n",
    "def check_entry_in_candidates(entry):\n",
    "    \"\"\"Checks if the entry start and ending tokens fall into a candidate.\n",
    "        Returns the candidate index, or -1 if none is found.\"\"\"\n",
    "    if not FLAGS.skip_nested_contexts:\n",
    "        raise NotImplementedError('Nested contexts have not been implemented for predictions yet!')\n",
    "    else:\n",
    "        for cand_idx, candidate in enumerate(entry['candidates']):\n",
    "            if not candidate['top_level']:\n",
    "                continue\n",
    "            if entry['orig_start'] >= candidate['start_token'] and \\\n",
    "                entry['orig_end'] <= candidate['end_token']:\n",
    "                return cand_idx\n",
    "    return -1\n",
    "\n",
    "def compute_answers(preds, candidates_dict, features, weights={}):\n",
    "    default_weights = {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0.0,\n",
    "        'conf_threshold': 0.98\n",
    "    }\n",
    "    \n",
    "    for k, v in default_weights.items():\n",
    "        if k not in weights:\n",
    "            weights[k] = v\n",
    "    \n",
    "    ### Get variables needed for post processing ###\n",
    "    start_token_probs = logits_to_probs(preds[0])\n",
    "    end_token_probs = logits_to_probs(preds[1])\n",
    "    ans_type_probs = logits_to_probs(preds[2])\n",
    "    candidates_dict = {int(k): v for k, v in candidates_dict.items()}\n",
    "\n",
    "    # Create a feature index to example_id mapping\n",
    "    f_idx_to_example_id = []\n",
    "    for feature in features:\n",
    "        f_idx_to_example_id.append(feature.unique_id - feature.doc_span_index)\n",
    "        \n",
    "    ### Create doc span groups ###\n",
    "    doc_span_groups = [(f_idx_to_example_id[0], [])]\n",
    "    for i in range(len(features)):\n",
    "        if f_idx_to_example_id[i] != doc_span_groups[-1][0]:\n",
    "            doc_span_groups.append((f_idx_to_example_id[i], []))\n",
    "            eval_example_idx += 1\n",
    "\n",
    "        eval_example_idx = len(doc_span_groups) - 1\n",
    "        group_data = {\n",
    "            'start_tokens_probs': start_token_probs[i],\n",
    "            'end_tokens_probs': end_token_probs[i],\n",
    "            'ans_type_probs': ans_type_probs[i],\n",
    "            'candidates': candidates_dict[f_idx_to_example_id[i]],\n",
    "            'feature': features[i],\n",
    "            'doc_tokens_map': eval_examples[eval_example_idx].doc_tokens_map\n",
    "        }\n",
    "\n",
    "        doc_span_groups[-1][1].append(group_data)\n",
    "    \n",
    "    ### Compute answers ###\n",
    "        \n",
    "    answers = {} # Maps example_ids to long and short answers\n",
    "    for example_id, group in doc_span_groups:\n",
    "        long_answer = None\n",
    "        short_answer = None\n",
    "\n",
    "        valid_entries = []\n",
    "        for entry in group:\n",
    "            # Converting logits to answer values\n",
    "            entry['start_token_idx'] = np.argmax(entry['start_tokens_probs'])\n",
    "            entry['end_token_idx'] = np.argmax(entry['end_tokens_probs'])\n",
    "            entry['ans_type_idx'] = np.argmax(entry['ans_type_probs'])\n",
    "\n",
    "            # TODO(Edan): Think about changing this to also check the probability\n",
    "            # of the token indices\n",
    "            # Calculating probability of the chosen answer type\n",
    "            entry['start_pos_prob'] = entry['start_tokens_probs'][entry['start_token_idx']]\n",
    "            entry['end_pos_prob'] = entry['end_tokens_probs'][entry['end_token_idx']]\n",
    "            entry['ans_type_prob'] = entry['ans_type_probs'][entry['ans_type_idx']]\n",
    "            \n",
    "            \n",
    "            entry['prob'] = weights['conf_bias'] + \\\n",
    "                            entry['start_pos_prob'] * weights['start_pos_conf_weight'] + \\\n",
    "                            entry['end_pos_prob'] * weights['end_pos_conf_weight'] + \\\n",
    "                            entry['ans_type_prob'] * weights['ans_type_conf_weight']\n",
    "            \n",
    "            # Filter out entries with invalid answers\n",
    "            if entry['end_token_idx'] < entry['start_token_idx'] or \\\n",
    "                (entry['end_token_idx'] - entry['start_token_idx'] > FLAGS.max_answer_length and \\\n",
    "                 entry['ans_type_idx'] == AnswerType.SHORT) or \\\n",
    "                entry['feature'].segment_ids[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].segment_ids[entry['end_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['end_token_idx']] == 0 or \\\n",
    "                entry['ans_type_idx'] == AnswerType.UNKNOWN or \\\n",
    "                entry['prob'] < weights['conf_threshold']:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Getting indices of tokens in original document\n",
    "                tok_to_orig_map = entry['feature'].token_to_orig_map\n",
    "                entry['orig_start'] = tok_to_orig_map[entry['start_token_idx']]\n",
    "                entry['orig_end'] = tok_to_orig_map[entry['end_token_idx']] + 1\n",
    "            except KeyError:\n",
    "                print('KEY ERROR BUT IGNORING FOR NOW')\n",
    "#                 print(entry['start_token_idx'])\n",
    "#                 print(entry['end_token_idx'])\n",
    "#                 print(tok_to_orig_map)\n",
    "                continue\n",
    "                \n",
    "            if entry['orig_start'] == -1 or entry['orig_end'] == 0:\n",
    "                continue\n",
    "\n",
    "            entry['candidate_idx'] = check_entry_in_candidates(entry)\n",
    "            if entry['candidate_idx'] == -1:\n",
    "                continue\n",
    "\n",
    "            valid_entries.append(entry)\n",
    "\n",
    "        if len(valid_entries) > 0:\n",
    "            # TODO(Edan): I think I should probably prioritize short answers\n",
    "            # over long answers because both can be right, but most long answer\n",
    "            # also have short answers\n",
    "            # I could also look at if other probabilities are also high,\n",
    "            # ideally only a single entry should have a high logit\n",
    "            best_entry = max(valid_entries, key=lambda e: e['prob'])\n",
    "            if best_entry['ans_type_idx'] == AnswerType.LONG:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.SHORT:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = (best_entry['orig_start'], best_entry['orig_end'])\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.YES:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'YES'\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.NO:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'NO'\n",
    "            else:\n",
    "                raise ValueError('Entry should not have AnswerType UNKNOWN or other!')\n",
    "\n",
    "        answers[example_id] = {'long_answer': long_answer,\n",
    "                               'short_answer': short_answer}\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.train_file, n=FLAGS.n_examples)\n",
    "else:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file, n=FLAGS.n_examples)\n",
    "\n",
    "answers = compute_answers(preds, candidates_dict, eval_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Computing the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_answers(file_path, n):\n",
    "    raw_generator = raw_data_generator(file_path, n)\n",
    "    raw_data = next(raw_generator)\n",
    "    \n",
    "    actual_answers = {}\n",
    "    for _, entry in raw_data.iterrows():\n",
    "        long_answer = entry['annotations'][0]['long_answer']\n",
    "        short_answers = entry['annotations'][0]['short_answers']\n",
    "        \n",
    "        la_spans = []\n",
    "        if long_answer['start_token'] != -1 and long_answer['end_token'] != -1:\n",
    "            la_spans = [(long_answer['start_token'], long_answer['end_token'])]\n",
    "        \n",
    "        sa_spans = []\n",
    "        for short_answer in short_answers:\n",
    "            sa_spans.append((short_answer['start_token'], short_answer['end_token']))\n",
    "            \n",
    "        if entry['annotations'][0]['yes_no_answer'] != 'NONE':\n",
    "            sa_spans.append(entry['annotations'][0]['yes_no_answer'])\n",
    "        \n",
    "        answer_entry = {\n",
    "            'long_answers': la_spans,\n",
    "            'short_answers': sa_spans\n",
    "        }\n",
    "        actual_answers[entry['example_id']] = answer_entry\n",
    "        \n",
    "    return actual_answers\n",
    "\n",
    "def score_preds(actual_answers, pred_answers):\n",
    "    if set(actual_answers.keys()) != set(answers.keys()):\n",
    "        raise ValueError('Actual answers and answers must contain the same example_id keys!')\n",
    "    \n",
    "    TP = 0 # True positives\n",
    "    FP = 0 # False positives\n",
    "    FN = 0 # False negatives\n",
    "    \n",
    "    FP_NA = 0 # False positives where there was no answer\n",
    "    FP_WA = 0 # False positives where an answer existed\n",
    "    for example_id in actual_answers.keys():\n",
    "        actual_las = actual_answers[example_id]['long_answers']\n",
    "        actual_sas = actual_answers[example_id]['short_answers']\n",
    "        \n",
    "        pred_la = pred_answers[example_id]['long_answer']\n",
    "        pred_sa = pred_answers[example_id]['short_answer']\n",
    "        \n",
    "        if pred_la in actual_las:\n",
    "            TP += 1\n",
    "        elif pred_la and pred_la not in actual_las:\n",
    "            FP += 1\n",
    "            if actual_las:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_la and actual_las:\n",
    "            FN += 1\n",
    "            \n",
    "        if pred_sa in actual_sas:\n",
    "            TP += 1\n",
    "        elif pred_sa and pred_sa not in actual_las:\n",
    "            FP += 1\n",
    "            if actual_sas:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_sa and actual_sas:\n",
    "            FN += 1\n",
    "\n",
    "    details = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'FP_WA': FP_WA,\n",
    "        'FP_NA': FP_NA\n",
    "    }\n",
    "    \n",
    "    if TP == 0:\n",
    "        return 0, 0, 0, details\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "    return micro_f1, recall, precision, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search to Select the Best Parameters for Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [05:17<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.4, 0.75, 0, -0.5, 0.5), (0.28703703703703703, 0.9841269841269841, 0.16802168021680217)), ((0.4, 0.75, 0, -0.25, 0.5), (0.2857142857142857, 0.9841269841269841, 0.16711590296495957)), ((0.4, 0.75, 0, 0, 0.5), (0.2857142857142857, 0.9841269841269841, 0.16711590296495957)), ((0.4, 0.75, 0, 0.25, 0.5), (0.2857142857142857, 0.9841269841269841, 0.16711590296495957)), ((0.4, 0.75, 0, 0.5, 0.5), (0.2857142857142857, 0.9841269841269841, 0.16711590296495957)), ((0.2, 1.0, 0, -0.5, 0.5), (0.28297362110311747, 0.9076923076923077, 0.16761363636363635)), ((0.2, 1.0, 0, -0.25, 0.5), (0.28297362110311747, 0.9076923076923077, 0.16761363636363635)), ((0.2, 1.0, 0, 0, 0.5), (0.28297362110311747, 0.9076923076923077, 0.16761363636363635)), ((0.2, 1.0, 0, 0.25, 0.5), (0.28297362110311747, 0.9076923076923077, 0.16761363636363635)), ((0.2, 1.0, 0, 0.5, 0.5), (0.28297362110311747, 0.9076923076923077, 0.16761363636363635)), ((0.6, 1.0, 0, -0.5, 0.5), (0.2811059907834101, 0.9838709677419355, 0.1639784946236559)), ((0.6, 1.0, 0, -0.25, 0.5), (0.2811059907834101, 0.9838709677419355, 0.1639784946236559)), ((0.6, 1.0, 0, 0, 0.5), (0.2811059907834101, 0.9838709677419355, 0.1639784946236559)), ((0.6, 1.0, 0, 0.25, 0.5), (0.2811059907834101, 0.9838709677419355, 0.1639784946236559)), ((0.6, 1.0, 0, 0.5, 0.5), (0.2811059907834101, 0.9838709677419355, 0.1639784946236559)), ((1.0, 1.0, 0, -0.5, 0.5), (0.28045977011494255, 0.9838709677419355, 0.16353887399463807)), ((1.0, 1.0, 0, -0.25, 0.5), (0.28045977011494255, 0.9838709677419355, 0.16353887399463807)), ((1.0, 1.0, 0, 0, 0.5), (0.28045977011494255, 0.9838709677419355, 0.16353887399463807)), ((1.0, 1.0, 0, 0.25, 0.5), (0.28045977011494255, 0.9838709677419355, 0.16353887399463807)), ((1.0, 1.0, 0, 0.5, 0.5), (0.28045977011494255, 0.9838709677419355, 0.16353887399463807))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    default_range = np.arange(0, 1.05, 0.5)\n",
    "\n",
    "    weight_ranges = OrderedDict({\n",
    "            'ans_type_conf_weight': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            'start_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'end_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'conf_bias': [-0.5, -0.25, 0, 0.25, 0.5],\n",
    "            'conf_threshold': [0.5]\n",
    "        })\n",
    "\n",
    "    combinations = list(itertools.product(*[weight_ranges[k] for k in weight_ranges.keys()]))\n",
    "\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "\n",
    "    results = {}\n",
    "    for weight_vals in tqdm.tqdm(combinations):\n",
    "        weights = {}\n",
    "        for weight_name, weight_val in zip(weight_ranges.keys(), weight_vals):\n",
    "                weights[weight_name] = weight_val\n",
    "\n",
    "        tmp_answers = compute_answers(preds, candidates_dict, eval_features, weights)\n",
    "        micro_f1, recall, precision, _ = score_preds(actual_answers, tmp_answers)\n",
    "        results[weight_vals] = (micro_f1, recall, precision)\n",
    "        \n",
    "    sr = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    print(sr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.100133  , -7.3039117 , -7.3831887 , -6.6233015 , -6.197779  ,\n",
       "       -7.1682525 , -6.3037863 , -6.715862  , -6.000416  , -6.4525876 ,\n",
       "       -5.056931  , -1.9543333 , -1.4809602 , -1.7070652 , -1.9236414 ,\n",
       "       -1.6917769 , -1.3885151 , -3.1001337 , -1.3020439 , -1.5146494 ,\n",
       "       -1.6497179 , -1.4559697 , -1.6309124 , -1.881473  , -1.7874224 ,\n",
       "       -1.9521971 , -1.9045782 , -1.8008068 , -1.7334101 , -1.975338  ,\n",
       "       -1.901504  , -1.8042233 , -2.2405949 , -1.8785186 , -1.8193161 ,\n",
       "       -2.0864377 , -1.9333713 , -1.9221984 , -2.0154371 , -1.994695  ,\n",
       "       -2.0238109 , -1.9131312 , -1.9161168 , -2.025159  , -2.0276756 ,\n",
       "       -2.0210612 , -1.8889117 , -1.8146737 , -1.9559637 , -2.0936298 ,\n",
       "       -2.002581  , -1.90209   , -2.030489  , -2.078301  , -2.125586  ,\n",
       "       -1.9358858 , -1.8182099 , -2.0710754 , -2.151546  , -1.9884982 ,\n",
       "       -1.8149433 , -2.0130777 , -2.0950632 , -2.0813346 , -2.1222663 ,\n",
       "       -2.0050187 , -1.8087535 , -2.0370798 , -2.065765  , -2.0898604 ,\n",
       "       -2.1061685 , -2.0846677 , -2.0875149 , -1.9234515 , -2.0581756 ,\n",
       "       -2.1462693 , -2.0670815 , -2.1325932 , -1.972703  , -1.9251075 ,\n",
       "       -2.0984755 , -2.134342  , -2.0779862 , -2.0685601 , -1.982759  ,\n",
       "       -2.1077795 , -2.1224327 , -2.1615565 , -2.114266  , -1.9582005 ,\n",
       "       -2.0768623 , -2.1130571 , -2.2102785 , -2.4136648 , -2.1361268 ,\n",
       "       -2.0770602 ,  1.8735318 , -2.0969098 , -2.0932655 , -1.8168197 ,\n",
       "       -1.2210293 , -0.662652  , -1.1627251 , -2.105142  , -0.06450021,\n",
       "       -2.0071826 , -1.8135473 , -2.1713755 , -1.880648  , -2.3340077 ,\n",
       "       -1.892355  , -1.9748335 , -1.8471193 , -2.1810827 , -2.173987  ,\n",
       "       -2.1482053 , -1.7445259 , -2.3480213 , -1.9594655 , -2.0273216 ,\n",
       "       -1.6625938 , -2.3419204 , -1.8908558 , -2.2364645 , -1.7652814 ,\n",
       "       -1.8711094 , -1.7965277 , -2.2862425 , -2.2267551 , -1.8377856 ,\n",
       "       -2.2176962 , -1.1944685 , -1.3542475 , -2.0546503 , -2.070703  ,\n",
       "       -2.0644648 , -2.0844665 , -2.0199704 , -2.2097163 , -2.1983356 ,\n",
       "       -2.1984763 , -2.283907  , -2.3025324 , -2.1508837 , -2.226247  ,\n",
       "       -2.0674455 , -2.2906432 , -1.6936612 , -2.306662  , -2.3523877 ,\n",
       "       -1.2975279 , -1.4550066 ,  2.5213518 , -1.5264924 ,  0.7732566 ,\n",
       "       -1.1716292 , -2.49711   , -2.0717201 , -2.3768866 , -1.2680914 ,\n",
       "       -2.066255  , -2.0214639 , -2.2472208 , -1.938329  , -1.3263443 ,\n",
       "       -2.262999  , -2.3366175 , -1.2141149 , -2.241668  , -2.3412406 ,\n",
       "       -2.0715308 , -2.3602104 , -2.334795  , -2.0478349 , -1.2176275 ,\n",
       "       -1.6620796 , -1.1581151 , -2.0984206 , -2.1994123 , -2.3233972 ,\n",
       "       -2.027315  , -2.2343183 , -2.2474363 , -2.3270621 , -2.2937255 ,\n",
       "       -2.4500206 , -2.2661767 , -2.6810703 , -2.0742385 , -2.3633127 ,\n",
       "       -2.3008082 , -2.3332615 , -2.4654908 , -2.2668962 , -2.50374   ,\n",
       "       -2.374106  , -2.4029284 , -2.3418252 , -2.4308977 , -2.354524  ,\n",
       "       -2.4250557 , -2.4268348 , -2.4271512 , -2.4356594 , -2.3584714 ,\n",
       "       -2.4160833 , -2.4014778 , -2.5107083 , -2.3869162 , -2.6547139 ,\n",
       "       -2.183676  , -2.4281375 , -2.3764474 , -1.6605259 , -2.214355  ,\n",
       "       -2.0933013 , -2.3322134 , -0.3499311 , -2.3928533 , -2.578404  ,\n",
       "       -2.3086696 , -2.2935114 , -2.3938103 , -2.54747   , -1.7660515 ,\n",
       "       -2.4523263 , -2.482781  , -2.4563062 , -2.568515  , -2.564311  ,\n",
       "       -2.5397563 , -2.4238904 , -2.6200306 , -2.5046182 , -2.577149  ,\n",
       "       -2.5209448 , -2.5844898 , -2.4962049 , -2.5785427 , -2.045097  ,\n",
       "       -2.4863641 , -2.4856086 , -2.7001023 , -2.5311484 , -2.5676165 ,\n",
       "       -2.521091  , -2.5888605 , -2.3001037 , -2.7294042 , -2.639248  ,\n",
       "       -2.6668944 , -2.2153144 , -2.711173  , -2.6326132 , -2.4053516 ,\n",
       "       -2.7763824 , -2.853819  , -3.0553312 , -2.592805  , -2.6562033 ,\n",
       "       -2.5876694 , -2.5687528 , -2.6275353 , -2.6209042 , -2.7640183 ,\n",
       "       -2.7080185 , -2.6810958 , -2.6075256 , -2.8907723 , -2.4267006 ,\n",
       "       -2.590264  , -2.4877565 , -2.6475506 , -2.7144015 , -2.6382961 ,\n",
       "       -2.6226883 , -2.6015344 , -2.6539993 , -2.678111  , -2.6724296 ,\n",
       "       -2.6313992 , -2.6935027 , -2.714315  , -2.681908  , -2.6820724 ,\n",
       "       -2.766432  , -2.6947796 , -2.6490133 , -2.7978373 , -2.629324  ,\n",
       "       -2.7328315 , -2.7296543 , -2.7893398 , -2.7587464 , -2.7232676 ,\n",
       "       -2.7363353 , -2.7184577 , -2.784175  , -2.7135856 , -2.7364602 ,\n",
       "       -2.7837086 , -2.668068  , -2.7402616 , -2.7770941 , -2.7081099 ,\n",
       "       -2.5718398 , -2.7820482 , -2.7853642 , -2.76047   , -2.7221541 ,\n",
       "       -2.8054795 , -2.7098565 , -2.7572002 , -2.7755156 , -2.7686057 ,\n",
       "       -2.805822  , -2.7342334 , -2.7316253 , -2.8626552 , -2.7154984 ,\n",
       "       -2.8823137 , -2.809287  , -2.7854233 , -2.7690434 , -2.881999  ,\n",
       "       -2.6672187 , -2.8634822 , -2.8744226 , -2.656627  , -2.8361406 ,\n",
       "       -2.8346963 , -2.8143783 , -2.9098544 , -2.885158  , -2.6807356 ,\n",
       "       -2.862729  , -2.8417315 , -2.903551  , -2.9237542 , -2.7098093 ,\n",
       "       -2.8681684 , -2.8692632 , -2.9009209 , -2.9147115 , -2.6966424 ,\n",
       "       -2.8685002 , -2.89909   , -2.9310489 , -2.9126675 , -2.781269  ,\n",
       "       -2.9097056 , -2.8688526 , -2.9533234 , -2.8409128 , -2.9520493 ,\n",
       "       -2.832683  , -2.9391217 , -2.8143    , -2.7970226 , -2.8942099 ,\n",
       "       -2.8746333 , -2.9562893 , -2.9735744 , -2.9728684 , -2.883495  ,\n",
       "       -2.963935  , -3.1126697 , -3.1332223 , -2.861858  , -3.0095568 ,\n",
       "       -3.0134797 , -3.0323942 , -3.1489005 , -2.8417134 , -3.0586677 ,\n",
       "       -3.1679516 , -3.3340793 , -3.0392716 , -3.1964414 , -3.3890023 ,\n",
       "       -3.860049  , -4.3580356 , -4.2287345 , -3.1001334 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 1.0, 0, -0.5, 0.5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.2, 1.0, 0, -0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17366946778711487,\n",
       " 0.4492753623188406,\n",
       " 0.1076388888888889,\n",
       " {'TP': 31, 'FP': 257, 'FN': 38, 'FP_WA': 123, 'FP_NA': 134})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_answers = compute_answers(preds, candidates_dict, eval_features, {\n",
    "        'ans_type_conf_weight': 0,\n",
    "        'start_pos_conf_weight': 0.5,\n",
    "        'end_pos_conf_weight': 0.5,\n",
    "        'conf_bias': 0,\n",
    "        'conf_threshold': 0.7\n",
    "    })\n",
    "actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "score_preds(actual_answers, tmp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual_answers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-08f4dccb7f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'conf_threshold'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.97\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     })\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscore_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'actual_answers' is not defined"
     ]
    }
   ],
   "source": [
    "tmp_answers = compute_answers(preds, candidates_dict, eval_features, {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0,\n",
    "        'conf_threshold': 0.97\n",
    "    })\n",
    "score_preds(actual_answers, tmp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5584415584415585 0.41030534351145037 0.8739837398373984\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN = 215, 460 - int(189*.8), 31\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "print(micro_f1, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0\n",
      "Recall: 0\n",
      "Precision: 0\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "    micro_f1, recall, precision, _ = score_preds(actual_answers, answers)\n",
    "\n",
    "    print(f'Micro F1 Score: {micro_f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(answers):\n",
    "    submission_data = []\n",
    "\n",
    "    # Loop through answers in alphabetic order of example_ids\n",
    "    # This is how it's sorted in the sample submission\n",
    "    for example_id, answer in sorted(answers.items(), key=lambda x: x[0]):\n",
    "        long_answer_text = ''\n",
    "        if isinstance(answer['long_answer'], tuple):\n",
    "            long_answer_text = f'{answer[\"long_answer\"][0]}:{answer[\"long_answer\"][1]}'\n",
    "        else:\n",
    "            assert answer['long_answer'] is None, 'Invalid type of long answer!'\n",
    "            assert answer['short_answer'] is None, 'Cannot have a short answer with no long answer!'\n",
    "        long_answer_row = [f'{example_id}_long', long_answer_text]\n",
    "\n",
    "        short_answer_text = ''\n",
    "        if isinstance(answer['short_answer'], tuple):\n",
    "            short_answer_text = f'{answer[\"short_answer\"][0]}:{answer[\"short_answer\"][1]}'\n",
    "        elif answer['short_answer'] in ('YES', 'NO'):\n",
    "            short_answer_text = answer['short_answer']\n",
    "        else:\n",
    "            assert answer['short_answer'] is None, 'Invalid type of short answer!'\n",
    "        short_answer_row = [f'{example_id}_short', short_answer_text]\n",
    "\n",
    "        submission_data.append(long_answer_row)\n",
    "        submission_data.append(short_answer_row)\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data, columns=['example_id', 'PredictionString'])\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save the Submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   example_id PredictionString\n",
      "0   -9111510312671706854_long        2000:2187\n",
      "1  -9111510312671706854_short        2087:2095\n",
      "2   -9100123296297706673_long          519:620\n",
      "3  -9100123296297706673_short          598:601\n",
      "4   -9070556881023521969_long        2882:3126\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.test_post_processing:\n",
    "    submission_df = create_submission(answers)\n",
    "    print(submission_df.head())\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
