{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ&A Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts.tf2_0_baseline_w_bert_translated_to_tf2_0 import AnswerType\n",
    "from scripts import bert_modeling as modeling\n",
    "from scripts import bert_tokenization\n",
    "from scripts import albert\n",
    "from scripts import albert_tokenization\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partial_nq_examples(input_file, is_training, n=-1):\n",
    "    \"\"\"Read a NQ json file into a list of NqExample.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_file)\n",
    "    input_data = []\n",
    "\n",
    "    def _open(path):\n",
    "        if path.endswith(\".gz\"):\n",
    "            return gzip.GzipFile(fileobj=tf.io.gfile.GFile(path, \"rb\"))\n",
    "        else:\n",
    "            return tf.io.gfile.GFile(path, \"r\")\n",
    "\n",
    "    for path in input_paths:\n",
    "        absl.logging.info(\"Reading: %s\", path)\n",
    "        with _open(path) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                input_data.append(tf2baseline.create_example_from_jsonl(line))\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        examples.extend(tf2baseline.read_nq_entry(entry, is_training))\n",
    "    return examples\n",
    "\n",
    "tf2baseline.read_nq_examples = read_partial_nq_examples\n",
    "\n",
    "def read_partial_candidates_from_one_split(input_path, n=-1):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                        \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                \n",
    "    return candidates_dict\n",
    "\n",
    "tf2baseline.read_candidates_from_one_split = read_partial_candidates_from_one_split\n",
    "\n",
    "def read_partial_candidates(input_pattern, n=-1):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for i, input_path in enumerate(input_paths):\n",
    "        final_dict.update(tf2baseline.read_candidates_from_one_split(input_path, n=n-len(final_dict.keys())))\n",
    "        if len(final_dict.keys()) >= n:\n",
    "                break\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "tf2baseline.read_candidates = read_partial_candidates\n",
    "\n",
    "def raw_data_generator(path, chunk_size=1000):\n",
    "        \"\"\"Reads raw JSON examples to a DataFrame\"\"\"\n",
    "        curr_pos = 0\n",
    "        last_line = False\n",
    "        with open(path, 'rt') as f:\n",
    "                while not last_line:\n",
    "                        df = []\n",
    "                        for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                                line = f.readline()\n",
    "                                if line is None:\n",
    "                                        last_line = True\n",
    "                                        break\n",
    "                                df.append(json.loads(line))\n",
    "                        curr_pos = i + 1\n",
    "                        yield pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"albert\",\n",
    "    \"The name of model to use. Choose from ['bert', 'albert'].\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xxl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the ALBERT/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", None,\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", -1,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_checkpoint_file\", None,\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"models/albert_xxl/albert_finetuned.h5\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 1, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 1,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", 1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "# TODO(Edan): Look at nested contents too at some point\n",
    "# Around 5% of long answers are nested, and around 50% of questions have\n",
    "# long answers\n",
    "# This means that this setting alone restricts us from a correct answer\n",
    "# around 2.5% of the time\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", 400,\n",
    "    \"Number of examples to read from files.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"test_post_processing\", True,\n",
    "    \"If true, training data will be predicted for instead of eval data,\"\n",
    "    \"and the predictions will be used to tune the post processing algorithm.\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                          \"non-floating point (and non-complex) \"\n",
    "                          \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                           \"`TDense` should be defined. \"\n",
    "                           \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
    "\n",
    "def get_bert_model(config_file):\n",
    "    \"\"\"Builds and returns a BERT model.\"\"\"\n",
    "    config = modeling.BertConfig.from_json_file(config_file)\n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    bert_layer = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n",
    "                                                input_mask=input_mask,\n",
    "                                                input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    return tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='bert_baseline')\n",
    "\n",
    "def get_albert_model(config_file):\n",
    "    \"\"\"Create an Albert model from pretrained configuration file with vocab_size changed\n",
    "    to 30522, and optionally loads the pretrained weights.\n",
    "    \"\"\"\n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    config.vocab_size = 30522\n",
    "    albert_layer = albert.AlbertModel(config=config)\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "    pooled_output, sequence_output = albert_layer(input_word_ids=input_ids,\n",
    "                                                    input_mask=input_mask,\n",
    "                                                    input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    albert_model = tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='albert')\n",
    "        \n",
    "    return albert_model\n",
    "\n",
    "def build_model(model_name, config_file):\n",
    "    \"\"\"Build model according to model_name.\n",
    "    \n",
    "    Args:\n",
    "        model_name: ['bert', 'albert']\n",
    "        config_file: path to config file\n",
    "        pretrain_ckpt: path to pretrain checkpoint (albert only)\n",
    "    Returns:\n",
    "        the specified model\n",
    "    \"\"\"\n",
    "    if model_name == 'albert':\n",
    "        model = get_albert_model(config_file)\n",
    "    elif model_name == 'bert':\n",
    "        model = get_bert_model(config_file)\n",
    "    else:\n",
    "        raise ValueError('{} is not supported'.format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"albert\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "albert_model (AlbertModel)      ((None, 4096), (None 222662400   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "td_seq (TDense)                 (None, 384, 2)       8194        albert_model[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 384, 1), (No 0           td_seq[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_logits (Tenso [(None, 384)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_logits (TensorF [(None, 384)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type_logits (TDense)        (None, 5)            20485       albert_model[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 222,691,079\n",
      "Trainable params: 222,691,079\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(FLAGS.model, FLAGS.config_file)\n",
    "model.load_weights(FLAGS.init_checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "Predict File: data/simplified-nq-train.jsonl\n",
      "**Features**\n",
      "\n",
      "answer_text\n",
      "answer_type\n",
      "doc_span_index\n",
      "end_position\n",
      "example_index\n",
      "input_ids\n",
      "input_mask\n",
      "segment_ids\n",
      "start_position\n",
      "token_is_max_context\n",
      "token_to_orig_map\n",
      "tokens\n",
      "unique_id\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.do_predict:\n",
    "    if FLAGS.model == 'bert':\n",
    "        tokenizer = bert_tokenization.FullTokenizer(\n",
    "            vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    elif FLAGS.model == 'albert':\n",
    "        tokenizer = albert_tokenization.FullTokenizer(\n",
    "            None, spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "    if FLAGS.test_post_processing:\n",
    "        input_file = FLAGS.train_file\n",
    "    else:\n",
    "        input_file = FLAGS.predict_file\n",
    "\n",
    "    # This is actually quite slow, but I'm not sure if it's due to tokenizing or reading from a compressed file\n",
    "    eval_examples = tf2baseline.read_nq_examples(\n",
    "          input_file=input_file, is_training=FLAGS.test_post_processing, n=FLAGS.n_examples)\n",
    "\n",
    "    print(len(eval_examples))\n",
    "\n",
    "    print(\"Predict File:\", input_file)\n",
    "\n",
    "    eval_writer = tf2baseline.FeatureWriter(\n",
    "      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "      is_training=FLAGS.test_post_processing)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        eval_features.append(feature)\n",
    "        eval_writer.process_feature(feature)\n",
    "\n",
    "    num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "      examples=eval_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      is_training=FLAGS.test_post_processing,\n",
    "      output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "    eval_filename = eval_writer.filename\n",
    "\n",
    "    print('**Features**\\n')\n",
    "\n",
    "    for e in dir(eval_features[0]):\n",
    "        if not e.startswith('__'):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **eval_features**\n",
    "- `doc_span_index`: Which index this span is in the set that makes up one question-article pair\n",
    "- `example_index`: Index of the example \n",
    "- `input_mask`: 0 if a token is padding, otherwise 1 for each token in the article\n",
    "- `segment_ids`: Tokens that are part of a question are 0 ([CLS], [Q], ..., [SEP])\n",
    "- `inpud_ids`: Token ids of the article\n",
    "    - 0 -> [PAD]\n",
    "    - 101 -> [CLS]\n",
    "    - 102 -> [SEP]\n",
    "    - 103 -> [MASK]\n",
    "    - 104 -> [Q]\n",
    "    - 105 -> [YES]\n",
    "    - 106 -> [NO]\n",
    "    - 107 -> [NoLongAnswer]\n",
    "    - 108 -> [NoShortAnswer]\n",
    "- `token_is_max_context`: False if this token will appear again in the next document span due to a sliding window being used, and True otherwise\n",
    "- `token_to_orig_map`: Maps the tokens in `tokens` and `input_ids` to the actual token indices in the original document\n",
    "- `tokens`: A list of tokens making up the document span\n",
    "- `unique_id`: A unique id number for this article (NOT this document span) \n",
    "\n",
    "*Note - mappings are 0 indexed and exlude tokens that are part of the question (those can be identified from segment ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def data_generator(batch_size=32, n_samples=-1, seed=42):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "#     if FLAGS.test_post_processing:\n",
    "#         eval_filename = FLAGS.train_precomputed_file\n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(eval_filename)\n",
    "    if FLAGS.do_train:\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=5000, seed=seed)\n",
    "        \n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    \n",
    "    data_iter = iter(dataset)\n",
    "    sample_idx = 0\n",
    "    while sample_idx < n_samples or n_samples == -1:\n",
    "        try:\n",
    "            examples = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        cutoff_amt = batch_size\n",
    "        if n_samples > -1:\n",
    "            cutoff_amt = min(cutoff_amt, n_samples - sample_idx)\n",
    "        sample_idx += cutoff_amt\n",
    "    \n",
    "        for k, v in examples.items():\n",
    "            examples[k] = v[:cutoff_amt]\n",
    "        \n",
    "        inputs = {\n",
    "            # 'unique_id': examples['unique_ids'],\n",
    "            'input_ids': examples['input_ids'],\n",
    "            'input_mask': examples['input_mask'],\n",
    "            'segment_ids': examples['segment_ids']\n",
    "        }\n",
    "\n",
    "        if FLAGS.do_train:\n",
    "            targets = {\n",
    "                'tf_op_layer_start_logits': examples['start_positions'],\n",
    "                'tf_op_layer_end_logits': examples['end_positions'],\n",
    "                'ans_type_logits': examples['answer_types'],\n",
    "            }\n",
    "\n",
    "            yield inputs, targets\n",
    "        else:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = np.ceil(eval_writer.num_features / FLAGS.predict_batch_size)\n",
    "generator = data_generator(FLAGS.predict_batch_size)\n",
    "\n",
    "preds = model.predict_generator(generator, steps=n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Types\n",
    "- UNKNOWN = 0\n",
    "- YES = 1\n",
    "- NO = 2\n",
    "- SHORT = 3\n",
    "- LONG = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Computing Answers from Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_probs(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "def get_candidate_span(entry):\n",
    "    return (entry['candidates'][entry['candidate_idx']]['start_token'],\n",
    "            entry['candidates'][entry['candidate_idx']]['end_token'])\n",
    "\n",
    "def check_entry_in_candidates(entry):\n",
    "    \"\"\"Checks if the entry start and ending tokens fall into a candidate.\n",
    "        Returns the candidate index, or -1 if none is found.\"\"\"\n",
    "    if not FLAGS.skip_nested_contexts:\n",
    "        raise NotImplementedError('Nested contexts have not been implemented for predictions yet!')\n",
    "    else:\n",
    "        for cand_idx, candidate in enumerate(entry['candidates']):\n",
    "            if not candidate['top_level']:\n",
    "                continue\n",
    "            if entry['orig_start'] >= candidate['start_token'] and \\\n",
    "                entry['orig_end'] <= candidate['end_token']:\n",
    "                return cand_idx\n",
    "    return -1\n",
    "\n",
    "def compute_answers(preds, candidates_dict, features, weights={}):\n",
    "    default_weights = {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0.0,\n",
    "        'conf_threshold': 0.98\n",
    "    }\n",
    "    \n",
    "    for k, v in default_weights.items():\n",
    "        if k not in weights:\n",
    "            weights[k] = v\n",
    "    \n",
    "    ### Get variables needed for post processing ###\n",
    "    start_token_probs = logits_to_probs(preds[0])\n",
    "    end_token_probs = logits_to_probs(preds[1])\n",
    "    ans_type_probs = logits_to_probs(preds[2])\n",
    "    candidates_dict = {int(k): v for k, v in candidates_dict.items()}\n",
    "\n",
    "    # Create a feature index to example_id mapping\n",
    "    f_idx_to_example_id = []\n",
    "    for feature in features:\n",
    "        f_idx_to_example_id.append(feature.unique_id - feature.doc_span_index)\n",
    "        \n",
    "    ### Create doc span groups ###\n",
    "    doc_span_groups = [(f_idx_to_example_id[0], [])]\n",
    "    for i in range(len(features)):\n",
    "        if f_idx_to_example_id[i] != doc_span_groups[-1][0]:\n",
    "            doc_span_groups.append((f_idx_to_example_id[i], []))\n",
    "            eval_example_idx += 1\n",
    "\n",
    "        eval_example_idx = len(doc_span_groups) - 1\n",
    "        group_data = {\n",
    "            'start_tokens_probs': start_token_probs[i],\n",
    "            'end_tokens_probs': end_token_probs[i],\n",
    "            'ans_type_probs': ans_type_probs[i],\n",
    "            'candidates': candidates_dict[f_idx_to_example_id[i]],\n",
    "            'feature': features[i],\n",
    "            'doc_tokens_map': eval_examples[eval_example_idx].doc_tokens_map\n",
    "        }\n",
    "\n",
    "        doc_span_groups[-1][1].append(group_data)\n",
    "    \n",
    "    ### Compute answers ###\n",
    "        \n",
    "    answers = {} # Maps example_ids to long and short answers\n",
    "    for example_id, group in doc_span_groups:\n",
    "        long_answer = None\n",
    "        short_answer = None\n",
    "\n",
    "        valid_entries = []\n",
    "        for entry in group:\n",
    "            # Converting logits to answer values\n",
    "            entry['start_token_idx'] = np.argmax(entry['start_tokens_probs'])\n",
    "            entry['end_token_idx'] = np.argmax(entry['end_tokens_probs'])\n",
    "            entry['ans_type_idx'] = np.argmax(entry['ans_type_probs'])\n",
    "\n",
    "            # TODO(Edan): Think about changing this to also check the probability\n",
    "            # of the token indices\n",
    "            # Calculating probability of the chosen answer type\n",
    "            entry['start_pos_prob'] = entry['start_tokens_probs'][entry['start_token_idx']]\n",
    "            entry['end_pos_prob'] = entry['end_tokens_probs'][entry['end_token_idx']]\n",
    "            entry['ans_type_prob'] = entry['ans_type_probs'][entry['ans_type_idx']]\n",
    "            \n",
    "            \n",
    "            entry['prob'] = weights['conf_bias'] + \\\n",
    "                            entry['start_pos_prob'] * weights['start_pos_conf_weight'] + \\\n",
    "                            entry['end_pos_prob'] * weights['end_pos_conf_weight'] + \\\n",
    "                            entry['ans_type_prob'] * weights['ans_type_conf_weight']\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Filter out entries with invalid answers\n",
    "            if entry['end_token_idx'] < entry['start_token_idx'] or \\\n",
    "                (entry['end_token_idx'] - entry['start_token_idx'] > FLAGS.max_answer_length and \\\n",
    "                 entry['ans_type_idx'] == AnswerType.SHORT) or \\\n",
    "                entry['feature'].segment_ids[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].segment_ids[entry['end_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['end_token_idx']] == 0 or \\\n",
    "                entry['ans_type_idx'] == AnswerType.UNKNOWN or \\\n",
    "                entry['prob'] < weights['conf_threshold']:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Getting indices of tokens in original document\n",
    "                tok_to_orig_map = entry['feature'].token_to_orig_map\n",
    "                entry['orig_start'] = tok_to_orig_map[entry['start_token_idx']]\n",
    "                entry['orig_end'] = tok_to_orig_map[entry['end_token_idx']] + 1\n",
    "            except KeyError:\n",
    "                print('KEY ERROR BUT IGNORING FOR NOW')\n",
    "#                 print(entry['start_token_idx'])\n",
    "#                 print(entry['end_token_idx'])\n",
    "#                 print(tok_to_orig_map)\n",
    "                continue\n",
    "                \n",
    "            if entry['orig_start'] == -1 or entry['orig_end'] == 0:\n",
    "                continue\n",
    "\n",
    "            entry['candidate_idx'] = check_entry_in_candidates(entry)\n",
    "            if entry['candidate_idx'] == -1:\n",
    "                continue\n",
    "\n",
    "            valid_entries.append(entry)\n",
    "\n",
    "        if len(valid_entries) > 0:\n",
    "            # TODO(Edan): I think I should probably prioritize short answers\n",
    "            # over long answers because both can be right, but most long answer\n",
    "            # also have short answers\n",
    "            best_entry = max(valid_entries, key=lambda e: e['prob'])\n",
    "            if best_entry['ans_type_idx'] == AnswerType.LONG:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.SHORT:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = (best_entry['orig_start'], best_entry['orig_end'])\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.YES:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'YES'\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.NO:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'NO'\n",
    "            else:\n",
    "                raise ValueError('Entry should not have AnswerType UNKNOWN or other!')\n",
    "\n",
    "        answers[example_id] = {'long_answer': long_answer,\n",
    "                               'short_answer': short_answer}\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.train_file, n=FLAGS.n_examples)\n",
    "else:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file, n=FLAGS.n_examples)\n",
    "\n",
    "answers = compute_answers(preds, candidates_dict, eval_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Computing the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_answers(file_path, n):\n",
    "    raw_generator = raw_data_generator(file_path, n)\n",
    "    raw_data = next(raw_generator)\n",
    "    \n",
    "    actual_answers = {}\n",
    "    for _, entry in raw_data.iterrows():\n",
    "        long_answer = entry['annotations'][0]['long_answer']\n",
    "        short_answers = entry['annotations'][0]['short_answers']\n",
    "        \n",
    "        la_spans = []\n",
    "        if long_answer['start_token'] != -1 and long_answer['end_token'] != -1:\n",
    "            la_spans = [(long_answer['start_token'], long_answer['end_token'])]\n",
    "        \n",
    "        sa_spans = []\n",
    "        for short_answer in short_answers:\n",
    "            sa_spans.append((short_answer['start_token'], short_answer['end_token']))\n",
    "            \n",
    "        if entry['annotations'][0]['yes_no_answer'] != 'NONE':\n",
    "            sa_spans.append(entry['annotations'][0]['yes_no_answer'])\n",
    "        \n",
    "        answer_entry = {\n",
    "            'long_answers': la_spans,\n",
    "            'short_answers': sa_spans\n",
    "        }\n",
    "        actual_answers[entry['example_id']] = answer_entry\n",
    "        \n",
    "    return actual_answers\n",
    "\n",
    "def score_preds(actual_answers, pred_answers):\n",
    "    if set(actual_answers.keys()) != set(answers.keys()):\n",
    "        raise ValueError('Actual answers and answers must contain the same example_id keys!')\n",
    "    \n",
    "    TP = 0 # True positives\n",
    "    FP = 0 # False positives\n",
    "    FN = 0 # False negatives\n",
    "    \n",
    "    FP_NA = 0 # False positives where there was no answer\n",
    "    FP_WA = 0 # False positives where an answer existed\n",
    "    for example_id in actual_answers.keys():\n",
    "        actual_las = actual_answers[example_id]['long_answers']\n",
    "        actual_sas = actual_answers[example_id]['short_answers']\n",
    "        \n",
    "        pred_la = pred_answers[example_id]['long_answer']\n",
    "        pred_sa = pred_answers[example_id]['short_answer']\n",
    "        \n",
    "        if pred_la in actual_las:\n",
    "            TP += 1\n",
    "        elif pred_la and pred_la not in actual_las:\n",
    "            FP += 1\n",
    "            if actual_las:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_la and actual_las:\n",
    "            FN += 1\n",
    "            \n",
    "        if pred_sa in actual_sas:\n",
    "            TP += 1\n",
    "        elif pred_sa and pred_sa not in actual_las:\n",
    "            FP += 1\n",
    "            FP += 1\n",
    "            if actual_sas:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_sa and actual_sas:\n",
    "            FN += 1\n",
    "\n",
    "    details = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'FP_WA': FP_WA,\n",
    "        'FP_NA': FP_NA\n",
    "    }\n",
    "    \n",
    "    if TP == 0:\n",
    "        return 0, 0, 0, details\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "    return micro_f1, recall, precision, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search to Select the Best Parameters for Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:04<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 0, 0, -0.5, 0.5), (0, 0, 0)), ((0, 0, 0, -0.25, 0.5), (0, 0, 0)), ((0, 0, 0, 0, 0.5), (0, 0, 0)), ((0, 0, 0, 0.25, 0.5), (0, 0, 0)), ((0, 0, 0, 0.5, 0.5), (0, 0, 0)), ((0, 0, 0.25, -0.5, 0.5), (0, 0, 0)), ((0, 0, 0.25, -0.25, 0.5), (0, 0, 0)), ((0, 0, 0.25, 0, 0.5), (0, 0, 0)), ((0, 0, 0.25, 0.25, 0.5), (0, 0, 0)), ((0, 0, 0.25, 0.5, 0.5), (0, 0, 0)), ((0, 0, 0.5, -0.5, 0.5), (0, 0, 0)), ((0, 0, 0.5, -0.25, 0.5), (0, 0, 0)), ((0, 0, 0.5, 0, 0.5), (0, 0, 0)), ((0, 0, 0.5, 0.25, 0.5), (0, 0, 0)), ((0, 0, 0.5, 0.5, 0.5), (0, 0, 0)), ((0, 0, 0.75, -0.5, 0.5), (0, 0, 0)), ((0, 0, 0.75, -0.25, 0.5), (0, 0, 0)), ((0, 0, 0.75, 0, 0.5), (0, 0, 0)), ((0, 0, 0.75, 0.25, 0.5), (0, 0, 0)), ((0, 0, 0.75, 0.5, 0.5), (0, 0, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    default_range = np.arange(0, 1.05, 0.5)\n",
    "\n",
    "    weight_ranges = OrderedDict({\n",
    "            'ans_type_conf_weight': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            'start_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'end_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'conf_bias': [-0.5, -0.25, 0, 0.25, 0.5],\n",
    "            'conf_threshold': [0.5]\n",
    "        })\n",
    "\n",
    "    combinations = list(itertools.product(*[weight_ranges[k] for k in weight_ranges.keys()]))\n",
    "\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "\n",
    "    results = {}\n",
    "    for weight_vals in tqdm.tqdm(combinations):\n",
    "        weights = {}\n",
    "        for weight_name, weight_val in zip(weight_ranges.keys(), weight_vals):\n",
    "                weights[weight_name] = weight_val\n",
    "\n",
    "        tmp_answers = compute_answers(preds, candidates_dict, eval_features, weights)\n",
    "        micro_f1, recall, precision, _ = score_preds(actual_answers, tmp_answers)\n",
    "        results[weight_vals] = (micro_f1, recall, precision)\n",
    "        \n",
    "    sr = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    print(sr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69155157, -6.414584  , -6.4029036 , -6.399822  , -6.404414  ,\n",
       "       -6.421957  , -6.311346  , -6.1578116 , -6.437191  , -6.429439  ,\n",
       "       -6.343073  , -6.031378  , -5.9306173 , -6.3493443 , -6.3855944 ,\n",
       "       -5.995136  , -5.8293104 , -5.5893946 , -6.1825066 , -6.284849  ,\n",
       "       -5.9423923 , -5.7058992 , -6.029213  , -6.2708297 , -6.1069727 ,\n",
       "       -6.187069  , -6.0942745 , -6.246569  , -6.2592697 , -6.2626967 ,\n",
       "       -6.2609606 , -6.248144  , -6.3566084 , -6.3654385 , -6.375697  ,\n",
       "       -6.3925505 , -6.2358584 , -6.257806  , -6.256448  , -6.2632914 ,\n",
       "       -6.2235637 , -6.2541933 , -6.257066  , -6.257488  , -6.2645264 ,\n",
       "       -6.216436  , -6.2469997 , -6.251031  , -6.259491  , -6.245411  ,\n",
       "       -6.129448  , -6.087572  , -6.2358685 , -6.25821   , -6.157945  ,\n",
       "       -6.1290326 , -6.0718794 , -6.2544684 , -6.256923  , -6.1763906 ,\n",
       "       -6.1426334 , -6.2555704 , -6.2571163 , -6.263612  , -6.20762   ,\n",
       "       -6.1987267 , -6.2543955 , -6.2576427 , -6.2621155 , -6.2242627 ,\n",
       "       -6.246497  , -6.2576294 , -6.2569065 , -6.264238  , -6.230398  ,\n",
       "       -6.257022  , -6.2583265 , -6.2576437 , -6.243864  , -6.170868  ,\n",
       "       -6.242223  , -6.2508583 , -6.2521887 , -6.130104  , -6.1162143 ,\n",
       "       -6.129965  , -6.232108  , -6.230859  , -6.292468  , -6.331503  ,\n",
       "       -6.325659  , -6.3290634 , -6.3434677 , -6.342794  , -6.3222594 ,\n",
       "       -6.32462   , -6.3471217 , -6.345545  , -6.338884  , -6.334281  ,\n",
       "       -6.337902  , -6.400011  , -6.433227  , -6.365678  , -6.346184  ,\n",
       "       -6.3449645 , -6.3850365 , -6.4400206 , -6.3672714 , -6.343767  ,\n",
       "       -5.9334855 , -6.122303  , -6.3940287 , -6.3517294 , -6.3411527 ,\n",
       "       -6.329277  , -6.392729  , -6.3947873 , -6.3807817 , -6.2023964 ,\n",
       "       -6.328479  , -6.325983  , -6.340934  , -6.359967  , -6.0127506 ,\n",
       "       -5.96758   , -6.3487844 , -6.367669  , -6.366618  , -6.031476  ,\n",
       "       -5.9970713 , -6.3964567 , -6.371429  , -6.386022  , -6.0632625 ,\n",
       "       -6.007291  , -6.2583323 , -6.432536  , -6.0294733 , -5.9745207 ,\n",
       "       -5.965415  , -5.982136  , -6.232616  , -6.199788  , -6.172016  ,\n",
       "       -6.1618066 , -6.223673  , -6.253804  , -6.2493095 , -6.197838  ,\n",
       "       -6.17586   , -6.2383933 , -6.2507195 , -6.204114  , -6.143535  ,\n",
       "       -6.077428  , -6.115163  , -6.227865  , -6.1853456 , -6.1234865 ,\n",
       "       -6.1551075 , -6.229535  , -6.232623  , -6.2297926 , -6.1630936 ,\n",
       "       -6.214491  , -6.258767  , -6.2374883 , -6.164077  , -6.1274586 ,\n",
       "       -6.161894  , -6.2414184 , -6.2127476 , -6.1703053 , -6.2151127 ,\n",
       "       -6.253537  , -6.2593365 , -6.234464  , -6.230788  , -6.2551546 ,\n",
       "       -6.255535  , -6.2604666 , -6.219856  , -6.2520404 , -6.255437  ,\n",
       "       -6.254556  , -6.246537  , -6.2270613 , -6.252013  , -6.253607  ,\n",
       "       -6.257037  , -6.2356486 , -6.2216625 , -6.2483835 , -6.2525268 ,\n",
       "       -6.256967  , -6.301591  , -6.301154  , -6.2833686 , -6.2953086 ,\n",
       "       -6.297901  , -6.291543  , -6.291208  , -6.285475  , -6.289492  ,\n",
       "       -6.2878275 , -6.2781677 , -6.254576  , -6.2299147 , -6.2458644 ,\n",
       "       -6.255044  , -6.2419915 , -6.2158403 , -6.215836  , -6.251162  ,\n",
       "       -6.2274427 , -6.230235  , -6.2113504 , -6.2200165 , -6.2490187 ,\n",
       "       -6.2320366 , -6.217478  , -6.205601  , -6.2166615 , -6.2445736 ,\n",
       "       -6.222494  , -6.1928396 , -6.1903267 , -6.189331  , -6.1869087 ,\n",
       "       -6.174932  , -6.17787   , -6.181059  , -6.1796637 , -6.145855  ,\n",
       "       -6.13407   , -6.168456  , -6.175603  , -6.311759  , -6.32659   ,\n",
       "       -6.314349  , -6.295311  , -6.3111305 , -6.318     , -6.3129516 ,\n",
       "       -6.304285  , -6.3042574 , -6.317214  , -6.315242  , -6.2894635 ,\n",
       "       -6.3040752 , -6.304179  , -6.3219337 , -6.3136377 , -6.303091  ,\n",
       "       -6.2971797 , -6.2905474 , -6.3078957 , -6.3054175 , -6.286602  ,\n",
       "       -6.2634892 , -6.265171  , -6.2587433 , -6.287056  , -6.2597833 ,\n",
       "       -6.2540417 , -6.238076  , -6.246853  , -6.255397  , -6.249623  ,\n",
       "       -6.264333  , -6.2509565 , -6.2581406 , -6.2480574 , -6.2535067 ,\n",
       "       -6.24916   , -6.250518  , -6.2321215 , -6.245372  , -6.2464    ,\n",
       "       -6.2579727 , -6.2675767 , -6.2411723 , -6.25078   , -6.267103  ,\n",
       "       -6.2648983 , -6.2395296 , -6.244585  , -6.2631054 , -6.256057  ,\n",
       "       -6.23792   , -6.231197  , -6.2447915 , -6.257077  , -6.244131  ,\n",
       "       -6.2392054 , -6.2525215 , -6.259984  , -6.2544413 , -6.24952   ,\n",
       "       -6.2539883 , -6.258426  , -6.25696   , -6.247378  , -6.243675  ,\n",
       "       -6.240448  , -6.237439  , -6.2356806 , -6.2322426 , -6.233416  ,\n",
       "       -6.2357454 , -6.2510448 , -6.2545557 , -6.256517  , -6.257145  ,\n",
       "       -6.2570095 , -6.2566633 , -6.261016  , -6.2548475 , -6.256102  ,\n",
       "       -6.254902  , -6.2597685 , -6.2578535 , -6.2543974 , -6.2546425 ,\n",
       "       -6.2596574 , -6.261405  , -6.255821  , -6.255632  , -6.256529  ,\n",
       "       -6.2621765 , -6.2600665 , -6.2563014 , -6.25658   , -6.261387  ,\n",
       "       -6.260652  , -6.2533393 , -6.252777  , -6.2534866 , -6.248538  ,\n",
       "       -6.242094  , -6.251915  , -6.252883  , -6.251539  , -6.2335663 ,\n",
       "       -6.247879  , -6.3358383 , -6.348032  , -6.3573813 , -6.3567376 ,\n",
       "       -6.3570733 , -6.36124   , -6.371249  , -6.374938  , -6.3741307 ,\n",
       "       -6.368949  , -6.3723426 , -6.3780913 , -6.383677  , -6.3806553 ,\n",
       "       -6.376487  , -6.3819494 , -6.403687  , -6.40046   , -6.376021  ,\n",
       "       -6.376069  , -6.3864784 , -6.3907094 , -6.378146  , -6.3566    ,\n",
       "       -6.367031  , -6.3825045 , -6.3906746 , -6.3723702 , -6.3368998 ,\n",
       "       -6.357351  , -6.3835144 , -6.382412  , -6.2586493 , -6.3472886 ,\n",
       "       -6.4101725 , -6.3689694 , -6.3798895 , -6.3748846 ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, {'TP': 0, 'FP': 0, 'FN': 373, 'FP_WA': 0, 'FP_NA': 0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_answers = compute_answers(preds, candidates_dict, eval_features, {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0,\n",
    "        'conf_threshold': 0.97\n",
    "    })\n",
    "score_preds(actual_answers, tmp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5584415584415585 0.41030534351145037 0.8739837398373984\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN = 215, 460 - int(189*.8), 31\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "print(micro_f1, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0\n",
      "Recall: 0\n",
      "Precision: 0\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "    micro_f1, recall, precision, _ = score_preds(actual_answers, answers)\n",
    "\n",
    "    print(f'Micro F1 Score: {micro_f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(answers):\n",
    "    submission_data = []\n",
    "\n",
    "    # Loop through answers in alphabetic order of example_ids\n",
    "    # This is how it's sorted in the sample submission\n",
    "    for example_id, answer in sorted(answers.items(), key=lambda x: x[0]):\n",
    "        long_answer_text = ''\n",
    "        if isinstance(answer['long_answer'], tuple):\n",
    "            long_answer_text = f'{answer[\"long_answer\"][0]}:{answer[\"long_answer\"][1]}'\n",
    "        else:\n",
    "            assert answer['long_answer'] is None, 'Invalid type of long answer!'\n",
    "            assert answer['short_answer'] is None, 'Cannot have a short answer with no long answer!'\n",
    "        long_answer_row = [f'{example_id}_long', long_answer_text]\n",
    "\n",
    "        short_answer_text = ''\n",
    "        if isinstance(answer['short_answer'], tuple):\n",
    "            short_answer_text = f'{answer[\"short_answer\"][0]}:{answer[\"short_answer\"][1]}'\n",
    "        elif answer['short_answer'] in ('YES', 'NO'):\n",
    "            short_answer_text = answer['short_answer']\n",
    "        else:\n",
    "            assert answer['short_answer'] is None, 'Invalid type of short answer!'\n",
    "        short_answer_row = [f'{example_id}_short', short_answer_text]\n",
    "\n",
    "        submission_data.append(long_answer_row)\n",
    "        submission_data.append(short_answer_row)\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data, columns=['example_id', 'PredictionString'])\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save the Submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   example_id PredictionString\n",
      "0   -9111510312671706854_long        2000:2187\n",
      "1  -9111510312671706854_short        2087:2095\n",
      "2   -9100123296297706673_long          519:620\n",
      "3  -9100123296297706673_short          598:601\n",
      "4   -9070556881023521969_long        2882:3126\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.test_post_processing:\n",
    "    submission_df = create_submission(answers)\n",
    "    print(submission_df.head())\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
