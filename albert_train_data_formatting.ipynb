{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT Q&A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts import albert_tokenization as tokenization\n",
    "\n",
    "import absl\n",
    "import json\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import tqdm\n",
    "import zipfile\n",
    "\n",
    "tf2baseline.FLAGS.include_unknowns = 1./20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the BERT/ALBERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"data/train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", 272565,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_checkpoint_file\", \"tf2_albert_finetuned.ckpt\",\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_predictions_file\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_init_checkpoint\", \"models/bert_joint_baseline/tf2_bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# This should be changed to 512 at some point,\n",
    "# as training was done with that value, it may\n",
    "# not make a big difference though\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 2, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", 1./20.,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "## Custom flags\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", -1,\n",
    "    \"Number of examples to read from files. Only applicable during testing\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl.zip\",\n",
    "    \"NQ json for training. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"albert_pretrain_checkpoint\", \"models/albert_xl/tf2_model.h5\",\n",
    "    \"Pretrain checkpoint (for Albert only).\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/home/ejmejm/MLProjects/nqa_kaggle/data/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Formatted Training Data (TFRecord, Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Examples: 307373\n"
     ]
    }
   ],
   "source": [
    "def blocks(f, size=65536):\n",
    "    while True:\n",
    "        b = f.read(size)\n",
    "        if not b:\n",
    "            break\n",
    "        yield b\n",
    "    \n",
    "with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "    with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "        n_train_examples = sum([bl.decode('UTF-8').count('\\n') for bl in blocks(f)])\n",
    "\n",
    "print('# Training Examples:', n_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = tokenization.FullTokenizer(\n",
    "#     None,\n",
    "#     spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "# tokenizer.tokenize('This this is a test [UNK] [ UNK] [Q]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "    with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "        line = f.readline().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</P>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(line)['document_text'].split(' ')[2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2baseline.create_example_from_jsonl(line, lowercase=True)['contexts'].split(' ')[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 5655493461695504401,\n",
       " 'id': '5655493461695504401',\n",
       " 'questions': [{'input_text': 'which is the most common use of opt-in e-mail marketing'}],\n",
       " 'answers': [{'candidate_id': 54,\n",
       "   'span_text': \"a newsletter sent to an advertising firm 's customers\",\n",
       "   'span_start': 10326,\n",
       "   'span_end': 10379,\n",
       "   'input_text': 'short'}],\n",
       " 'has_correct_context': True,\n",
       " 'contexts': \"[ContextId=-1] [NoLongAnswer] [ContextId=0] [Table=1] ( hide ) this article has multiple issues . please help improve it or discuss these issues on the talk page . ( learn how and when to remove these template messages ) this article needs additional citations for verification . please help improve this article by adding citations to reliable sources . unsourced material may be challenged and removed . ( september 2014 ) ( learn how and when to remove this template message ) this article possibly contains original research . please improve it by verifying the claims made and adding inline citations . statements consisting only of original research should be removed . ( january 2015 ) ( learn how and when to remove this template message ) ( learn how and when to remove this template message ) [ContextId=6] [Table=2] part of a series on internet marketing search engine optimization local search engine optimisation social media marketing email marketing referral marketing content marketing native advertising search engine marketing pay - per - click cost per impression search analytics web analytics display advertising ad blocking contextual advertising behavioral targeting affiliate marketing cost per action revenue sharing mobile advertising [ContextId=29] [Paragraph=1] email marketing is the act of sending a commercial message , typically to a group of people , using email . in its broadest sense , every email sent to a potential or current customer could be considered email marketing . it usually involves using email to send advertisements , request business , or solicit sales or donations , and is meant to build loyalty , trust , or brand awareness . marketing emails can be sent to a purchased lead list or a current customer database . the term usually refers to sending email messages with the purpose of enhancing a merchant 's relationship with current or previous customers , encouraging customer loyalty and repeat business , acquiring new customers or convincing current customers to purchase something immediately , and sharing third - party ads . [ContextId=30] [Paragraph=2] email marketing has evolved rapidly alongside the technological growth of the 21st century . prior to this growth , when emails were novelties to the majority of customers , email marketing was not as effective . in 1978 , gary thuerk of digital equipment corporation ( dec ) sent out the first mass email to approximately 400 potential clients via the advanced research projects agency network ( arpanet ) . this email resulted in $13 million worth of sales in dec products , and highlighted the potential of marketing through mass emails . however , as email marketing developed as an effective means of direct communication , users began blocking out content from emails with filters and blocking programs . in order to effectively communicate a message through email , marketers had to develop a way of pushing content through to the end user , without being cut out by automatic filters and spam removing software . this resulted in the birth of triggered marketing emails , which are sent to specific users based on their tracked online browsing patterns . [ContextId=31] [Paragraph=3] historically , it has been difficult to measure the effectiveness of marketing campaigns because target markets can not be adequately defined . email marketing carries the benefit of allowing marketers to identify returns on investment and measure and improve efficiency . email marketing allows marketers to see feedback from users in real time , and to monitor how effective their campaign is in achieving market penetration , revealing a communication channel 's scope . at the same time , however , it also means that the more personal nature of certain advertising methods , such as television advertisements , can not be captured . [ContextId=32] [Paragraph=4] email marketing can be carried out through different types of emails : [ContextId=33] [Paragraph=5] transactional emails are usually triggered based on a customer 's action with a company . to be qualified as transactional or relationship messages , these communications ' primary purpose must be `` to facilitate , complete , or confirm a commercial transaction that the recipient has previously agreed to enter into with the sender '' along with a few other narrow definitions of transactional messaging . triggered transactional messages include dropped basket messages , password reset emails , purchase or order confirmation emails , order status emails , reorder emails , and email receipts . [ContextId=34] [Paragraph=6] the primary purpose of a transactional email is to convey information regarding the action that triggered it . but , due to their high open rates ( 51.3 % compared to 36.6 % for email newsletters ) , transactional emails are an opportunity to introduce or extend the email relationship with customers or subscribers ; to anticipate and answer questions ; or to cross-sell or up - sell products or services . [ContextId=35] [Paragraph=7] many email newsletter software vendors offer transactional email support , which gives companies the ability to include promotional messages within the body of transactional emails . there are also software vendors that offer specialized transactional email marketing services , which include providing targeted and personalized transactional email messages and running specific marketing campaigns ( such as customer referral programs ) . [ContextId=36] [Paragraph=8] direct email involves sending an email solely to communicate a promotional message ( for example , a special offer or a product catalog ) . companies usually collect a list of customer or prospect email addresses to send direct promotional messages to , or they rent a list of email addresses from service companies . safe mail marketing is also used . [ContextId=37] [Paragraph=9] email marketing develops large amounts of traffic through smartphones and tablets . marketers are researching ways to advertise to more users and to make them view advertising for longer . however , the rate of delivery is still relatively low due to better filtering - out of advertising and users having multiple email accounts for different purposes . because emails are generated according to the tracked behavior of consumers , it is possible to send advertising which is based on the recipient 's behavior . because of this , modern email marketing is perceived more often as a pull strategy rather than a push strategy . [ContextId=38] [Paragraph=10] there are both advantages and disadvantages to using email marketing in comparison to traditional advertising mail . [ContextId=39] [Paragraph=11] email marketing is popular with companies for several reasons : [ContextId=40] [List=1] an exact return on investment can be tracked ( `` track to basket '' ) and has proven to be high when done properly . email marketing is often reported as second only to search marketing as the most effective online marketing tactic . email marketing is significantly cheaper and faster than traditional mail , mainly because of the high cost and time required in a traditional mail campaign for producing the artwork , printing , addressing , and mailing . businesses and organizations who send a high volume of emails can use an esp ( email service provider ) to gather information about the behavior of the recipients . the insights provided by consumer response to email marketing help businesses and organizations understand and make use of consumer behavior . email provides a cost - effective method to test different marketing content , including visual , creative , marketing copy , and multimedia assets . the data gathered by testing in the email channel can then be used across all channels of marketing campaigns , both print and digital . advertisers can reach substantial numbers of email subscribers who have opted in ( i.e. , consented ) to receive the email . almost half of american internet users check or send email on a typical day , with emails delivered between 1 am and 5 am local time outperforming those sent at other times in open and click rates . email is popular with digital marketers , rising an estimated 15 % in 2009 to £ 292 million in the uk . if compared to standard email , direct email marketing produces higher response rate and higher average order value for e-commerce businesses . [ContextId=49] [Paragraph=12] as of mid-2016 email deliverability is still an issue for legitimate marketers . according to the report , legitimate email servers averaged a delivery rate of 73 % in the u.s. ; six percent were filtered as spam , and 22 % were missing . this lags behind other countries : australia delivers at 90 % , canada at 89 % , britain at 88 % , france at 84 % , germany at 80 % and brazil at 79 % . [ContextId=50] [Paragraph=13] additionally , consumers receive on average circa 90 emails per day . [ContextId=51] [Paragraph=14] companies considering the use of an email marketing program must make sure that their program does not violate spam laws such as the united states ' controlling the assault of non-solicited pornography and marketing act ( can - spam ) , the european privacy and electronic communications regulations 2003 , or their internet service provider 's acceptable use policy . [ContextId=52] [Paragraph=15] opt - in email advertising , or permission marketing , is a method of advertising via email whereby the recipient of the advertisement has consented to receive it . this method is one of several developed by marketers to eliminate the disadvantages of email marketing . [ContextId=53] [Paragraph=16] opt - in email marketing may evolve into a technology that uses a handshake protocol between the sender and receiver . this system is intended to eventually result in a high degree of satisfaction between consumers and marketers . if opt - in email advertising is used , the material that is emailed to consumers will be `` anticipated '' . it is assumed that the recipient wants to receive it , which makes it unlike unsolicited advertisements sent to the consumer . ideally , opt - in email advertisements will be more personal and relevant to the consumer than untargeted advertisements . [ContextId=54] [Paragraph=17] a common example of permission marketing is a newsletter sent to an advertising firm 's customers . such newsletters inform customers of upcoming events or promotions , or new products . in this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . [ContextId=55] [Paragraph=18] with a foundation of opted - in contact information stored in their database , marketers can send out promotional materials automatically using autoresponders -- known as drip marketing . they can also segment their promotions to specific market segments . [ContextId=56] [Paragraph=19] the australian spam act 2003 is enforced by the australian communications and media authority , widely known as `` acma '' . the act defines the term unsolicited electronic messages , states how unsubscribe functions must work for commercial messages , and gives other key information . fines range with 3 fines of au $110,000 being issued to virgin blue airlines ( 2011 ) , tiger airways holdings limited ( 2012 ) and cellar master wines pty limited ( 2013 ) . [ContextId=57] [Paragraph=20] the `` canada anti-spam law '' ( casl ) went into effect on july 1 , 2014 . casl requires an explicit or implicit opt - in from users , and the maximum fines for noncompliance are ca $ 1 million for individuals and $10 million for businesses . [ContextId=58] [Paragraph=21] in 2002 the european union ( eu ) introduced the directive on privacy and electronic communications . article 13 of the directive prohibits the use of personal email addresses for marketing purposes . the directive establishes the opt - in regime , where unsolicited emails may be sent only with prior agreement of the recipient ; this does not apply to business email addresses . [ContextId=59] [Paragraph=22] the directive has since been incorporated into the laws of member states . in the uk it is covered under the privacy and electronic communications ( ec directive ) regulations 2003 and applies to all organizations that send out marketing by some form of electronic communication . [ContextId=60] [Paragraph=23] the can - spam act of 2003 was passed by congress as a direct response of the growing number of complaints over spam e-mails . congress determined that the us government was showing an increased interest in the regulation of commercial electronic mail nationally , that those who send commercial e-mails should not mislead recipients over the source or content of them , and that all recipients of such emails have a right to decline them . the act authorizes a us $16,000 penalty per violation for spamming each individual recipient . however , it does not ban spam emailing outright , but imposes laws on using deceptive marketing methods through headings which are `` materially false or misleading '' . in addition there are conditions which email marketers must meet in terms of their format , their content and labeling . as a result , many commercial email marketers within the united states utilize a service or special software to ensure compliance with the act . a variety of older systems exist that do not ensure compliance with the act . to comply with the act 's regulation of commercial email , services also typically require users to authenticate their return address and include a valid physical address , provide a one - click unsubscribe feature , and prohibit importing lists of purchased addresses that may not have given valid permission . [ContextId=61] [Paragraph=24] in addition to satisfying legal requirements , email service providers ( esps ) began to help customers establish and manage their own email marketing campaigns . the service providers supply email templates and general best practices , as well as methods for handling subscriptions and cancellations automatically . some esps will provide insight and assistance with deliverability issues for major email providers . they also provide statistics pertaining to the number of messages received and opened , and whether the recipients clicked on any links within the messages . [ContextId=62] [Paragraph=25] the can - spam act was updated with some new regulations including a no - fee provision for opting out , further definition of `` sender '' , post office or private mail boxes count as a `` valid physical postal address '' and definition of `` person '' . these new provisions went into effect on july 7 , 2008 .\",\n",
       " 'contexts_map': [-1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  -1,\n",
       "  -1,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  182,\n",
       "  183,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  206,\n",
       "  207,\n",
       "  210,\n",
       "  211,\n",
       "  214,\n",
       "  215,\n",
       "  218,\n",
       "  219,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  247,\n",
       "  248,\n",
       "  251,\n",
       "  252,\n",
       "  259,\n",
       "  260,\n",
       "  267,\n",
       "  268,\n",
       "  271,\n",
       "  272,\n",
       "  275,\n",
       "  276,\n",
       "  283,\n",
       "  284,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  296,\n",
       "  297,\n",
       "  304,\n",
       "  305,\n",
       "  -1,\n",
       "  -1,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  -1,\n",
       "  -1,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  -1,\n",
       "  -1,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  -1,\n",
       "  -1,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  -1,\n",
       "  -1,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  -1,\n",
       "  -1,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  1000,\n",
       "  1001,\n",
       "  1002,\n",
       "  1003,\n",
       "  1004,\n",
       "  1005,\n",
       "  1006,\n",
       "  1007,\n",
       "  1008,\n",
       "  1009,\n",
       "  1010,\n",
       "  1011,\n",
       "  1012,\n",
       "  1013,\n",
       "  1014,\n",
       "  1015,\n",
       "  1016,\n",
       "  1017,\n",
       "  1018,\n",
       "  1019,\n",
       "  1020,\n",
       "  1021,\n",
       "  1022,\n",
       "  1023,\n",
       "  1024,\n",
       "  1025,\n",
       "  1026,\n",
       "  1027,\n",
       "  1028,\n",
       "  1029,\n",
       "  1030,\n",
       "  1031,\n",
       "  1032,\n",
       "  1033,\n",
       "  1034,\n",
       "  1035,\n",
       "  1036,\n",
       "  1037,\n",
       "  1038,\n",
       "  1039,\n",
       "  1040,\n",
       "  1041,\n",
       "  1042,\n",
       "  -1,\n",
       "  -1,\n",
       "  1045,\n",
       "  1046,\n",
       "  1047,\n",
       "  1048,\n",
       "  1049,\n",
       "  1050,\n",
       "  1051,\n",
       "  1052,\n",
       "  1053,\n",
       "  1054,\n",
       "  1055,\n",
       "  1056,\n",
       "  1057,\n",
       "  1058,\n",
       "  1059,\n",
       "  1060,\n",
       "  1061,\n",
       "  1062,\n",
       "  1063,\n",
       "  1064,\n",
       "  1065,\n",
       "  1066,\n",
       "  1067,\n",
       "  1068,\n",
       "  1069,\n",
       "  1070,\n",
       "  1071,\n",
       "  1072,\n",
       "  1073,\n",
       "  1074,\n",
       "  1075,\n",
       "  1076,\n",
       "  1077,\n",
       "  1078,\n",
       "  1079,\n",
       "  1080,\n",
       "  1081,\n",
       "  1082,\n",
       "  1083,\n",
       "  1084,\n",
       "  1085,\n",
       "  1086,\n",
       "  1087,\n",
       "  1088,\n",
       "  1089,\n",
       "  1090,\n",
       "  1091,\n",
       "  1092,\n",
       "  1093,\n",
       "  1094,\n",
       "  1095,\n",
       "  1096,\n",
       "  1097,\n",
       "  1098,\n",
       "  1099,\n",
       "  1100,\n",
       "  1101,\n",
       "  1102,\n",
       "  1103,\n",
       "  1104,\n",
       "  1105,\n",
       "  -1,\n",
       "  -1,\n",
       "  1112,\n",
       "  1113,\n",
       "  1114,\n",
       "  1115,\n",
       "  1116,\n",
       "  1117,\n",
       "  1118,\n",
       "  1119,\n",
       "  1120,\n",
       "  1121,\n",
       "  1122,\n",
       "  1123,\n",
       "  1124,\n",
       "  1125,\n",
       "  1126,\n",
       "  1127,\n",
       "  1128,\n",
       "  1129,\n",
       "  1130,\n",
       "  1131,\n",
       "  1132,\n",
       "  1133,\n",
       "  1134,\n",
       "  1135,\n",
       "  1136,\n",
       "  1137,\n",
       "  1138,\n",
       "  1139,\n",
       "  1140,\n",
       "  1141,\n",
       "  1142,\n",
       "  1143,\n",
       "  1144,\n",
       "  1145,\n",
       "  1146,\n",
       "  1147,\n",
       "  1148,\n",
       "  1149,\n",
       "  1150,\n",
       "  1151,\n",
       "  1152,\n",
       "  1153,\n",
       "  1154,\n",
       "  1155,\n",
       "  1156,\n",
       "  1157,\n",
       "  1158,\n",
       "  1159,\n",
       "  1160,\n",
       "  1161,\n",
       "  1162,\n",
       "  1163,\n",
       "  1164,\n",
       "  1165,\n",
       "  1166,\n",
       "  1167,\n",
       "  1168,\n",
       "  1169,\n",
       "  1170,\n",
       "  1171,\n",
       "  1172,\n",
       "  1173,\n",
       "  -1,\n",
       "  -1,\n",
       "  1179,\n",
       "  1180,\n",
       "  1181,\n",
       "  1182,\n",
       "  1183,\n",
       "  1184,\n",
       "  1185,\n",
       "  1186,\n",
       "  1187,\n",
       "  1188,\n",
       "  1189,\n",
       "  1190,\n",
       "  1191,\n",
       "  1192,\n",
       "  1193,\n",
       "  1194,\n",
       "  1195,\n",
       "  1196,\n",
       "  1197,\n",
       "  1198,\n",
       "  1199,\n",
       "  1200,\n",
       "  1201,\n",
       "  1202,\n",
       "  1203,\n",
       "  1204,\n",
       "  1205,\n",
       "  1206,\n",
       "  1207,\n",
       "  1208,\n",
       "  1209,\n",
       "  1210,\n",
       "  1211,\n",
       "  1212,\n",
       "  1213,\n",
       "  1214,\n",
       "  1215,\n",
       "  1216,\n",
       "  1217,\n",
       "  1218,\n",
       "  1219,\n",
       "  1220,\n",
       "  1221,\n",
       "  1222,\n",
       "  1223,\n",
       "  1224,\n",
       "  1225,\n",
       "  1226,\n",
       "  1227,\n",
       "  1228,\n",
       "  1229,\n",
       "  1230,\n",
       "  1231,\n",
       "  1232,\n",
       "  1233,\n",
       "  1234,\n",
       "  1235,\n",
       "  1236,\n",
       "  1237,\n",
       "  1238,\n",
       "  1239,\n",
       "  1240,\n",
       "  1241,\n",
       "  ...]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2baseline.create_example_from_jsonl(line, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(chunk_size=1000):\n",
    "    curr_pos = 0\n",
    "    last_line = False\n",
    "    with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "        with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "            while not last_line:\n",
    "                examples = []\n",
    "                for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                    line = f.readline().decode('UTF-8')\n",
    "                    if line is None:\n",
    "                        last_line = True\n",
    "                        break\n",
    "                    examples.append(tf2baseline.create_example_from_jsonl(line, lowercase=True))\n",
    "                    examples[-1] = tf2baseline.read_nq_entry(examples[-1], FLAGS.do_train)[0]\n",
    "                curr_pos = i + 1\n",
    "                yield examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 614/615 [30:28:45<02:58, 178.71s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Features written: 783611\n",
      "\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aadab19fad3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Create new threads to replace finished ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-466d65877feb>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mlast_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf2baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_example_from_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf2baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nq_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mcurr_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLProjects/kaggle-nqa/scripts/tf2_0_baseline_w_bert_translated_to_tf2_0.py\u001b[0m in \u001b[0;36mcreate_example_from_jsonl\u001b[0;34m(line, lowercase)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_example_from_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[0;34m\"\"\"Creates an NQ example from a given line of JSON.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m   \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;31m# Added by Edan to support all lowercase for ALBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "example_gen = data_generator(chunk_size=chunk_size)\n",
    "\n",
    "train_writer = tf2baseline.FeatureWriter(\n",
    "    filename=os.path.join(FLAGS.output_dir, \"albert_train.tf_record\"),\n",
    "    is_training=FLAGS.do_train)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    None,\n",
    "    spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "def append_feature(feature, lock):\n",
    "    with lock:\n",
    "        train_writer.process_feature(feature)\n",
    "    \n",
    "def create_features(examples, lock):\n",
    "    num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        is_training=FLAGS.do_train,\n",
    "        output_fn=lambda x: append_feature(x, lock))\n",
    "\n",
    "\n",
    "\n",
    "n_steps = int(np.ceil(n_train_examples/chunk_size))\n",
    "n_threads = 1\n",
    "lock = threading.Lock()\n",
    "threads = []\n",
    "try:\n",
    "    curr_step = 0\n",
    "    pbar = tqdm.tqdm(total=n_steps)\n",
    "    while curr_step < n_steps:\n",
    "        # Get rid of threads that have finished\n",
    "        for thread in threads:\n",
    "            if not thread.isAlive():\n",
    "                thread.handled = True\n",
    "                pbar.update(1)\n",
    "        threads = [thread for thread in threads if not thread.handled]\n",
    "        \n",
    "        # Create new threads to replace finished ones\n",
    "        if len(threads) < n_threads:\n",
    "            thread = threading.Thread(target=create_features, args=(next(example_gen), lock))\n",
    "            thread.handled = False\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            curr_step += 1\n",
    "        \n",
    "        with open('output/albert_finished_loop_idx.txt', 'w+') as f:\n",
    "            f.write(str(curr_step))\n",
    "\n",
    "        time.sleep(0.05)\n",
    "finally:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    train_writer._writer.flush()\n",
    "    train_writer.close()\n",
    "    train_filename = train_writer.filename\n",
    "    pbar.close()\n",
    "\n",
    "    print(f'# Features written: {train_writer.num_features}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
