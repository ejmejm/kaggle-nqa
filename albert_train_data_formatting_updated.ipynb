{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT Q&A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts import albert_tokenization as tokenization\n",
    "\n",
    "import absl\n",
    "import collections\n",
    "import json\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import tqdm\n",
    "import zipfile\n",
    "\n",
    "tf2baseline.FLAGS.include_unknowns = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the BERT/ALBERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"albert_train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "## Custom flags\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", -1,\n",
    "    \"Number of examples to read from files. Only applicable during testing\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl.zip\",\n",
    "    \"NQ json for training. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/home/ejmejm/MLProjects/nqa_kaggle/data/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Formatted Training Data (TFRecord, Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Examples: 307373\n"
     ]
    }
   ],
   "source": [
    "def blocks(f, size=65536):\n",
    "    while True:\n",
    "        b = f.read(size)\n",
    "        if not b:\n",
    "            break\n",
    "        yield b\n",
    "    \n",
    "with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "    with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "        n_train_examples = sum([bl.decode('UTF-8').count('\\n') for bl in blocks(f)])\n",
    "\n",
    "print('# Training Examples:', n_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = tokenization.FullTokenizer(\n",
    "#     None,\n",
    "#     spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "# tokenizer.tokenize('This this is a test [UNK] [ UNK] [Q]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ids = []\n",
    "with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "    with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().decode('UTF-8')\n",
    "            if line is None:\n",
    "                last_line = True\n",
    "                break\n",
    "            example = json.loads(line, object_pairs_hook=collections.OrderedDict)\n",
    "            example_ids.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_ids) - len(set(example_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(chunk_size=1000):\n",
    "    curr_pos = 0\n",
    "    last_line = False\n",
    "    with zipfile.ZipFile(FLAGS.train_file) as zip_file:\n",
    "        with zip_file.open('simplified-nq-train.jsonl', 'r') as f:\n",
    "            while not last_line:\n",
    "                examples = []\n",
    "                for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                    line = f.readline().decode('UTF-8')\n",
    "                    if line is None:\n",
    "                        last_line = True\n",
    "                        break\n",
    "                    examples.append(tf2baseline.create_example_from_jsonl(line, lowercase=True))\n",
    "                    examples[-1] = tf2baseline.read_nq_entry(examples[-1], FLAGS.do_train)[0]\n",
    "                curr_pos = i + 1\n",
    "                yield examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 351/615 [16:58:02<13:14:57, 180.67s/it]"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "example_gen = data_generator(chunk_size=chunk_size)\n",
    "\n",
    "train_writer = tf2baseline.FeatureWriter(\n",
    "    filename=os.path.join(FLAGS.output_dir, FLAGS.train_precomputed_file),\n",
    "    is_training=FLAGS.do_train)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    None,\n",
    "    spm_model_file=FLAGS.vocab_file)\n",
    "\n",
    "def append_feature(feature, lock):\n",
    "    with lock:\n",
    "        train_writer.process_feature(feature)\n",
    "    \n",
    "def create_features(examples, lock):\n",
    "    num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        is_training=FLAGS.do_train,\n",
    "        output_fn=lambda x: append_feature(x, lock))\n",
    "\n",
    "\n",
    "\n",
    "n_steps = int(np.ceil(n_train_examples/chunk_size))\n",
    "n_threads = 1\n",
    "lock = threading.Lock()\n",
    "threads = []\n",
    "try:\n",
    "    curr_step = 0\n",
    "    pbar = tqdm.tqdm(total=n_steps)\n",
    "    while curr_step < n_steps:\n",
    "        # Get rid of threads that have finished\n",
    "        for thread in threads:\n",
    "            if not thread.isAlive():\n",
    "                thread.handled = True\n",
    "                pbar.update(1)\n",
    "        threads = [thread for thread in threads if not thread.handled]\n",
    "        \n",
    "        # Create new threads to replace finished ones\n",
    "        if len(threads) < n_threads:\n",
    "            thread = threading.Thread(target=create_features, args=(next(example_gen), lock))\n",
    "            thread.handled = False\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            curr_step += 1\n",
    "        \n",
    "        with open('output/albert_finished_loop_idx_2.txt', 'w+') as f:\n",
    "            f.write(str(curr_step))\n",
    "\n",
    "        time.sleep(0.05)\n",
    "finally:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    train_writer._writer.flush()\n",
    "    train_writer.close()\n",
    "    train_filename = train_writer.filename\n",
    "    pbar.close()\n",
    "\n",
    "    print(f'# Features written: {train_writer.num_features}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Format the Dataset as Needed but Faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntf = {\n",
    "       'input_ids': tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "       'input_mask': tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "       'segment_ids': tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "       'start_positions': tf.io.FixedLenFeature([], tf.int64),\n",
    "       'end_positions': tf.io.FixedLenFeature([], tf.int64),\n",
    "       'answer_types': tf.io.FixedLenFeature([], tf.int64)\n",
    "      }\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "    return example\n",
    "\n",
    "def encode_example(features):\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def create_int_feature(values):\n",
    "    feature = tf.train.Feature(\n",
    "        int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Records: 783611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383.0 [2:40:15<00:00, 25.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset('data/' + FLAGS.train_precomputed_file)\n",
    "n_records = 0\n",
    "for records in dataset.batch(100000):\n",
    "    n_records += len(records)\n",
    "print('# Records:', n_records)\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=n_records, seed=42)\n",
    "dataset = dataset.map(lambda x: decode_record(x, ntf))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "\n",
    "n_has_answers = 0\n",
    "n_no_answers = 0\n",
    "n_target_no_answers = 180000\n",
    "\n",
    "writer = tf.io.TFRecordWriter(os.path.join(FLAGS.output_dir, 'new_records.tf_record'))\n",
    "\n",
    "try:\n",
    "    pbar = tqdm.tqdm(total=np.ceil(n_records/batch_size))\n",
    "    for record_batch in dataset:\n",
    "        for i in range(len(record_batch['answer_types'])):\n",
    "            record_data = collections.OrderedDict()\n",
    "            for k in record_batch.keys():\n",
    "                if record_batch[k][i].shape == ():\n",
    "                    record_data[k] = create_int_feature([record_batch[k][i].numpy()])\n",
    "                else:\n",
    "                    record_data[k] = create_int_feature(record_batch[k][i].numpy())\n",
    "            answer_type = record_data['answer_types'].int64_list.value[0]\n",
    "            if n_no_answers < n_target_no_answers and \\\n",
    "               answer_type == tf2baseline.AnswerType.UNKNOWN:\n",
    "                writer.write(encode_example(record_data))\n",
    "                n_no_answers += 1\n",
    "            elif answer_type != tf2baseline.AnswerType.UNKNOWN:\n",
    "                writer.write(encode_example(record_data))\n",
    "                n_has_answers += 1\n",
    "        pbar.update(1)\n",
    "finally:\n",
    "    writer.close()\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
