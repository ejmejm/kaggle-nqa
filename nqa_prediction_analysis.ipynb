{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ&A Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts.tf2_0_baseline_w_bert_translated_to_tf2_0 import AnswerType\n",
    "from scripts import bert_modeling as modeling\n",
    "from scripts import bert_optimization as optimization\n",
    "from scripts import bert_tokenization as tokenization\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partial_nq_examples(input_file, is_training, n=-1):\n",
    "    \"\"\"Read a NQ json file into a list of NqExample.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_file)\n",
    "    input_data = []\n",
    "\n",
    "    def _open(path):\n",
    "        if path.endswith(\".gz\"):\n",
    "            return gzip.GzipFile(fileobj=tf.io.gfile.GFile(path, \"rb\"))\n",
    "        else:\n",
    "            return tf.io.gfile.GFile(path, \"r\")\n",
    "\n",
    "    for path in input_paths:\n",
    "        absl.logging.info(\"Reading: %s\", path)\n",
    "        with _open(path) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                input_data.append(tf2baseline.create_example_from_jsonl(line))\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        examples.extend(tf2baseline.read_nq_entry(entry, is_training))\n",
    "    return examples\n",
    "\n",
    "tf2baseline.read_nq_examples = read_partial_nq_examples\n",
    "\n",
    "def read_partial_candidates_from_one_split(input_path, n=-1):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            absl.logging.info(\"Reading examples from: %s\", input_path)\n",
    "            for index, line in enumerate(input_file):\n",
    "                if n > -1 and index >= n:\n",
    "                        break\n",
    "                        \n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                \n",
    "    return candidates_dict\n",
    "\n",
    "tf2baseline.read_candidates_from_one_split = read_partial_candidates_from_one_split\n",
    "\n",
    "def read_partial_candidates(input_pattern, n=-1):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for i, input_path in enumerate(input_paths):\n",
    "        final_dict.update(tf2baseline.read_candidates_from_one_split(input_path, n=n-len(final_dict.keys())))\n",
    "        if len(final_dict.keys()) >= n:\n",
    "                break\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "tf2baseline.read_candidates = read_partial_candidates\n",
    "\n",
    "def raw_data_generator(path, chunk_size=1000):\n",
    "        \"\"\"Reads raw JSON examples to a DataFrame\"\"\"\n",
    "        curr_pos = 0\n",
    "        last_line = False\n",
    "        with open(path, 'rt') as f:\n",
    "                while not last_line:\n",
    "                        df = []\n",
    "                        for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                                line = f.readline()\n",
    "                                if line is None:\n",
    "                                        last_line = True\n",
    "                                        break\n",
    "                                df.append(json.loads(line))\n",
    "                        curr_pos = i + 1\n",
    "                        yield pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", \"models/bert_joint_baseline/bert_config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/bert_joint_baseline/vocab-nq.txt\",\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_prediction_file\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"models/bert_joint_baseline/tf2_bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"data/train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", 10,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", 1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "# TODO(Edan): Look at nested contents too at some point\n",
    "# Around 5% of long answers are nested, and around 50% of questions have\n",
    "# long answers\n",
    "# This means that this setting alone restricts us from a correct answer\n",
    "# around 2.5% of the time\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", 400,\n",
    "    \"Number of examples to read from files.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"test_post_processing\", True,\n",
    "    \"If true, training data will be predicted for instead of eval data,\"\n",
    "    \"and the predictions will be used to tune the post processing algorithm.\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                          \"non-floating point (and non-complex) \"\n",
    "                          \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                           \"`TDense` should be defined. \"\n",
    "                           \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
    "\n",
    "def build_model(bert_config):\n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "    bert_layer = modeling.BertModel(config=bert_config, name='bert')\n",
    "    pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n",
    "                                                input_mask=input_mask,\n",
    "                                                input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    return tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='bert_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in converted code:\n\n    /home/ejmejm/MLProjects/kaggle-nqa/scripts/bert_modeling.py:386 call  *\n        word_embeddings = self.embedding_lookup(input_word_ids)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    /home/ejmejm/MLProjects/kaggle-nqa/scripts/bert_modeling.py:431 build\n        dtype=self.dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:522 add_weight\n        aggregation=aggregation)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable\n        shape=variable_shape if variable_shape else None)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:258 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call\n        shape=shape)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:197 <lambda>\n        previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py:2507 default_variable_creator\n        shape=shape)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:262 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1406 __init__\n        distribute_strategy=distribute_strategy)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1537 _init_from_args\n        initial_value() if init_from_fn else initial_value,\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:119 <lambda>\n        init_val = lambda: initializer(shape, dtype=dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py:343 __call__\n        self.stddev, dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py:809 truncated_normal\n        shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py:175 truncated_normal\n        shape_tensor, dtype, seed=seed1, seed2=seed2)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_random_ops.py:1006 truncated_normal\n        _six.raise_from(_core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[30522,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TruncatedNormal]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ef6ca150e5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .assert_consumed() will not work right now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3c6534f74a74>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(bert_config)\u001b[0m\n\u001b[1;32m     47\u001b[0m     pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n\u001b[1;32m     48\u001b[0m                                                 \u001b[0minput_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                                                 input_type_ids=segment_ids)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Maybe try sharing the start and end logits variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLProjects/kaggle-nqa/scripts/bert_modeling.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_word_ids, input_mask, input_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m                **kwargs):\n\u001b[1;32m    367\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in converted code:\n\n    /home/ejmejm/MLProjects/kaggle-nqa/scripts/bert_modeling.py:386 call  *\n        word_embeddings = self.embedding_lookup(input_word_ids)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    /home/ejmejm/MLProjects/kaggle-nqa/scripts/bert_modeling.py:431 build\n        dtype=self.dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:522 add_weight\n        aggregation=aggregation)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\n        **kwargs_for_getter)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable\n        shape=variable_shape if variable_shape else None)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:258 __call__\n        return cls._variable_v1_call(*args, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call\n        shape=shape)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:197 <lambda>\n        previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py:2507 default_variable_creator\n        shape=shape)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:262 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1406 __init__\n        distribute_strategy=distribute_strategy)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1537 _init_from_args\n        initial_value() if init_from_fn else initial_value,\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:119 <lambda>\n        init_val = lambda: initializer(shape, dtype=dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py:343 __call__\n        self.stddev, dtype)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py:809 truncated_normal\n        shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py:175 truncated_normal\n        shape_tensor, dtype, seed=seed1, seed2=seed2)\n    /home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_random_ops.py:1006 truncated_normal\n        _six.raise_from(_core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[30522,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TruncatedNormal]\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "model = build_model(bert_config)\n",
    "model.load_weights(FLAGS.init_checkpoint) # .assert_consumed() will not work right now\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "FLAGS.predict_file /kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "**Features**\n",
      "\n",
      "answer_text\n",
      "answer_type\n",
      "doc_span_index\n",
      "end_position\n",
      "example_index\n",
      "input_ids\n",
      "input_mask\n",
      "segment_ids\n",
      "start_position\n",
      "token_is_max_context\n",
      "token_to_orig_map\n",
      "tokens\n",
      "unique_id\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.do_predict:\n",
    "    if not FLAGS.output_prediction_file:\n",
    "        raise ValueError(\n",
    "            \"--output_prediction_file must be defined in predict mode.\")\n",
    "    \n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    if FLAGS.output_prediction_file:\n",
    "        input_file = FLAGS.train_file\n",
    "    else:\n",
    "        input_file = FLAGS.predict_file\n",
    "\n",
    "    # This is actually quite slow, but I'm not sure if it's due to tokenizing or reading from a compressed file\n",
    "    eval_examples = tf2baseline.read_nq_examples(\n",
    "          input_file=input_file, is_training=FLAGS.test_post_processing, n=FLAGS.n_examples)\n",
    "\n",
    "    print(len(eval_examples))\n",
    "\n",
    "    print(\"FLAGS.predict_file\", FLAGS.predict_file)\n",
    "\n",
    "    eval_writer = tf2baseline.FeatureWriter(\n",
    "      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "      is_training=FLAGS.test_post_processing)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        eval_features.append(feature)\n",
    "        eval_writer.process_feature(feature)\n",
    "\n",
    "    num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "      examples=eval_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      is_training=FLAGS.test_post_processing,\n",
    "      output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "    eval_filename = eval_writer.filename\n",
    "\n",
    "    print('**Features**\\n')\n",
    "\n",
    "    for e in dir(eval_features[0]):\n",
    "        if not e.startswith('__'):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **eval_features**\n",
    "- `doc_span_index`: Which index this span is in the set that makes up one question-article pair\n",
    "- `example_index`: Index of the example \n",
    "- `input_mask`: 0 if a token is padding, otherwise 1 for each token in the article\n",
    "- `segment_ids`: Tokens that are part of a question are 0 ([CLS], [Q], ..., [SEP])\n",
    "- `inpud_ids`: Token ids of the article\n",
    "    - 0 -> [PAD]\n",
    "    - 101 -> [CLS]\n",
    "    - 102 -> [SEP]\n",
    "    - 103 -> [MASK]\n",
    "    - 104 -> [Q]\n",
    "    - 105 -> [YES]\n",
    "    - 106 -> [NO]\n",
    "    - 107 -> [NoLongAnswer]\n",
    "    - 108 -> [NoShortAnswer]\n",
    "- `token_is_max_context`: False if this token will appear again in the next document span due to a sliding window being used, and True otherwise\n",
    "- `token_to_orig_map`: Maps the tokens in `tokens` and `input_ids` to the actual token indices in the original document\n",
    "- `tokens`: A list of tokens making up the document span\n",
    "- `unique_id`: A unique id number for this article (NOT this document span) \n",
    "\n",
    "*Note - mappings are 0 indexed and exlude tokens that are part of the question (those can be identified from segment ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a newsletter sent to an advertising firm 's customers\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples[0].answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which is the most common use of opt-in e-mail marketing']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples[0].questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f59ac2803d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "for example in eval_examples:\n",
    "    if example.answer.type in (AnswerType.LONG, AnswerType.YES, AnswerType.NO):\n",
    "        answers.append(example.answer.text)\n",
    "    elif example.answer.type == AnswerType.SHORT:\n",
    "        answers.append(example.answer.text)\n",
    "    \n",
    "answer_lens = [len(x) for x in answers]\n",
    "sns.distplot(answer_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2959.3599999999988"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(answer_lens, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def data_generator(batch_size=32, n_samples=-1, seed=42):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "#     if FLAGS.test_post_processing:\n",
    "#         eval_filename = FLAGS.train_precomputed_file\n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(eval_filename)\n",
    "    if FLAGS.do_train:\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=5000, seed=seed)\n",
    "        \n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    \n",
    "    data_iter = iter(dataset)\n",
    "    sample_idx = 0\n",
    "    while sample_idx < n_samples or n_samples == -1:\n",
    "        try:\n",
    "            examples = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        cutoff_amt = batch_size\n",
    "        if n_samples > -1:\n",
    "            cutoff_amt = min(cutoff_amt, n_samples - sample_idx)\n",
    "        sample_idx += cutoff_amt\n",
    "    \n",
    "        for k, v in examples.items():\n",
    "            examples[k] = v[:cutoff_amt]\n",
    "        \n",
    "        inputs = {\n",
    "            # 'unique_id': examples['unique_ids'],\n",
    "            'input_ids': examples['input_ids'],\n",
    "            'input_mask': examples['input_mask'],\n",
    "            'segment_ids': examples['segment_ids']\n",
    "        }\n",
    "\n",
    "        if FLAGS.do_train:\n",
    "            targets = {\n",
    "                'tf_op_layer_start_logits': examples['start_positions'],\n",
    "                'tf_op_layer_end_logits': examples['end_positions'],\n",
    "                'ans_type_logits': examples['answer_types'],\n",
    "            }\n",
    "\n",
    "            yield inputs, targets\n",
    "        else:\n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = np.ceil(eval_writer.num_features / FLAGS.predict_batch_size)\n",
    "generator = data_generator(FLAGS.predict_batch_size)\n",
    "\n",
    "preds = model.predict_generator(generator, steps=n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Types\n",
    "- UNKNOWN = 0\n",
    "- YES = 1\n",
    "- NO = 2\n",
    "- SHORT = 3\n",
    "- LONG = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Computing Answers from Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_probs(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "def get_candidate_span(entry):\n",
    "    return (entry['candidates'][entry['candidate_idx']]['start_token'],\n",
    "            entry['candidates'][entry['candidate_idx']]['end_token'])\n",
    "\n",
    "def check_entry_in_candidates(entry):\n",
    "    \"\"\"Checks if the entry start and ending tokens fall into a candidate.\n",
    "        Returns the candidate index, or -1 if none is found.\"\"\"\n",
    "    if not FLAGS.skip_nested_contexts:\n",
    "        raise NotImplementedError('Nested contexts have not been implemented for predictions yet!')\n",
    "    else:\n",
    "        for cand_idx, candidate in enumerate(entry['candidates']):\n",
    "            if not candidate['top_level']:\n",
    "                continue\n",
    "            if entry['orig_start'] >= candidate['start_token'] and \\\n",
    "                entry['orig_end'] <= candidate['end_token']:\n",
    "                return cand_idx\n",
    "    return -1\n",
    "\n",
    "def compute_answers(preds, candidates_dict, features, weights={}):\n",
    "    default_weights = {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0.0,\n",
    "        'conf_threshold': 0.98\n",
    "    }\n",
    "    \n",
    "    for k, v in default_weights.items():\n",
    "        if k not in weights:\n",
    "            weights[k] = v\n",
    "    \n",
    "    ### Get variables needed for post processing ###\n",
    "    start_token_probs = logits_to_probs(preds[0])\n",
    "    end_token_probs = logits_to_probs(preds[1])\n",
    "    ans_type_probs = logits_to_probs(preds[2])\n",
    "    candidates_dict = {int(k): v for k, v in candidates_dict.items()}\n",
    "\n",
    "    # Create a feature index to example_id mapping\n",
    "    f_idx_to_example_id = []\n",
    "    for feature in features:\n",
    "        f_idx_to_example_id.append(feature.unique_id - feature.doc_span_index)\n",
    "        \n",
    "    ### Create doc span groups ###\n",
    "    doc_span_groups = [(f_idx_to_example_id[0], [])]\n",
    "    for i in range(len(features)):\n",
    "        if f_idx_to_example_id[i] != doc_span_groups[-1][0]:\n",
    "            doc_span_groups.append((f_idx_to_example_id[i], []))\n",
    "            eval_example_idx += 1\n",
    "\n",
    "        eval_example_idx = len(doc_span_groups) - 1\n",
    "        group_data = {\n",
    "            'start_tokens_probs': start_token_probs[i],\n",
    "            'end_tokens_probs': end_token_probs[i],\n",
    "            'ans_type_probs': ans_type_probs[i],\n",
    "            'candidates': candidates_dict[f_idx_to_example_id[i]],\n",
    "            'feature': features[i],\n",
    "            'doc_tokens_map': eval_examples[eval_example_idx].doc_tokens_map\n",
    "        }\n",
    "\n",
    "        doc_span_groups[-1][1].append(group_data)\n",
    "    \n",
    "    ### Compute answers ###\n",
    "        \n",
    "    answers = {} # Maps example_ids to long and short answers\n",
    "    for example_id, group in doc_span_groups:\n",
    "        long_answer = None\n",
    "        short_answer = None\n",
    "\n",
    "        valid_entries = []\n",
    "        for entry in group:\n",
    "            # Converting logits to answer values\n",
    "            entry['start_token_idx'] = np.argmax(entry['start_tokens_probs'])\n",
    "            entry['end_token_idx'] = np.argmax(entry['end_tokens_probs'])\n",
    "            entry['ans_type_idx'] = np.argmax(entry['ans_type_probs'])\n",
    "\n",
    "            # TODO(Edan): Think about changing this to also check the probability\n",
    "            # of the token indices\n",
    "            # Calculating probability of the chosen answer type\n",
    "            entry['start_pos_prob'] = entry['start_tokens_probs'][entry['start_token_idx']]\n",
    "            entry['end_pos_prob'] = entry['end_tokens_probs'][entry['end_token_idx']]\n",
    "            entry['ans_type_prob'] = entry['ans_type_probs'][entry['ans_type_idx']]\n",
    "            \n",
    "            \n",
    "            entry['prob'] = weights['conf_bias'] + \\\n",
    "                            entry['start_pos_prob'] * weights['start_pos_conf_weight'] + \\\n",
    "                            entry['end_pos_prob'] * weights['end_pos_conf_weight'] + \\\n",
    "                            entry['ans_type_prob'] * weights['ans_type_conf_weight']\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Filter out entries with invalid answers\n",
    "            if entry['end_token_idx'] < entry['start_token_idx'] or \\\n",
    "                (entry['end_token_idx'] - entry['start_token_idx'] > FLAGS.max_answer_length and \\\n",
    "                 entry['ans_type_idx'] == AnswerType.SHORT) or \\\n",
    "                entry['feature'].segment_ids[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].segment_ids[entry['end_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['start_token_idx']] == 0 or \\\n",
    "                entry['feature'].input_mask[entry['end_token_idx']] == 0 or \\\n",
    "                entry['ans_type_idx'] == AnswerType.UNKNOWN or \\\n",
    "                entry['prob'] < weights['conf_threshold']:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Getting indices of tokens in original document\n",
    "                tok_to_orig_map = entry['feature'].token_to_orig_map\n",
    "                entry['orig_start'] = tok_to_orig_map[entry['start_token_idx']]\n",
    "                entry['orig_end'] = tok_to_orig_map[entry['end_token_idx']] + 1\n",
    "            except KeyError:\n",
    "                print('KEY ERROR BUT IGNORING FOR NOW')\n",
    "#                 print(entry['start_token_idx'])\n",
    "#                 print(entry['end_token_idx'])\n",
    "#                 print(tok_to_orig_map)\n",
    "                continue\n",
    "                \n",
    "            if entry['orig_start'] == -1 or entry['orig_end'] == 0:\n",
    "                continue\n",
    "\n",
    "            entry['candidate_idx'] = check_entry_in_candidates(entry)\n",
    "            if entry['candidate_idx'] == -1:\n",
    "                continue\n",
    "\n",
    "            valid_entries.append(entry)\n",
    "\n",
    "        if len(valid_entries) > 0:\n",
    "            # TODO(Edan): I think I should probably prioritize short answers\n",
    "            # over long answers because both can be right, but most long answer\n",
    "            # also have short answers\n",
    "            best_entry = max(valid_entries, key=lambda e: e['prob'])\n",
    "            if best_entry['ans_type_idx'] == AnswerType.LONG:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.SHORT:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = (best_entry['orig_start'], best_entry['orig_end'])\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.YES:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'YES'\n",
    "            elif best_entry['ans_type_idx'] == AnswerType.NO:\n",
    "                long_answer = get_candidate_span(best_entry)\n",
    "                short_answer = 'NO'\n",
    "            else:\n",
    "                raise ValueError('Entry should not have AnswerType UNKNOWN or other!')\n",
    "\n",
    "        answers[example_id] = {'long_answer': long_answer,\n",
    "                               'short_answer': short_answer}\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.train_file, n=FLAGS.n_examples)\n",
    "else:\n",
    "    candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file, n=FLAGS.n_examples)\n",
    "\n",
    "answers = compute_answers(preds, candidates_dict, eval_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Computing the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_answers(file_path, n):\n",
    "    raw_generator = raw_data_generator(file_path, n)\n",
    "    raw_data = next(raw_generator)\n",
    "    \n",
    "    actual_answers = {}\n",
    "    for _, entry in raw_data.iterrows():\n",
    "        long_answer = entry['annotations'][0]['long_answer']\n",
    "        short_answers = entry['annotations'][0]['short_answers']\n",
    "        \n",
    "        la_spans = []\n",
    "        if long_answer['start_token'] != -1 and long_answer['end_token'] != -1:\n",
    "            la_spans = [(long_answer['start_token'], long_answer['end_token'])]\n",
    "        \n",
    "        sa_spans = []\n",
    "        for short_answer in short_answers:\n",
    "            sa_spans.append((short_answer['start_token'], short_answer['end_token']))\n",
    "            \n",
    "        if entry['annotations'][0]['yes_no_answer'] != 'NONE':\n",
    "            sa_spans.append(entry['annotations'][0]['yes_no_answer'])\n",
    "        \n",
    "        answer_entry = {\n",
    "            'long_answers': la_spans,\n",
    "            'short_answers': sa_spans\n",
    "        }\n",
    "        actual_answers[entry['example_id']] = answer_entry\n",
    "        \n",
    "    return actual_answers\n",
    "\n",
    "def score_preds(actual_answers, pred_answers):\n",
    "    if set(actual_answers.keys()) != set(answers.keys()):\n",
    "        raise ValueError('Actual answers and answers must contain the same example_id keys!')\n",
    "    \n",
    "    TP = 0 # True positives\n",
    "    FP = 0 # False positives\n",
    "    FN = 0 # False negatives\n",
    "    \n",
    "    FP_NA = 0 # False positives where there was no answer\n",
    "    FP_WA = 0 # False positives where an answer existed\n",
    "    for example_id in actual_answers.keys():\n",
    "        actual_las = actual_answers[example_id]['long_answers']\n",
    "        actual_sas = actual_answers[example_id]['short_answers']\n",
    "        \n",
    "        pred_la = pred_answers[example_id]['long_answer']\n",
    "        pred_sa = pred_answers[example_id]['short_answer']\n",
    "        \n",
    "        if pred_la in actual_las:\n",
    "            TP += 1\n",
    "        elif pred_la and pred_la not in actual_las:\n",
    "            FP += 1\n",
    "            if actual_las:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_la and actual_las:\n",
    "            FN += 1\n",
    "            \n",
    "        if pred_sa in actual_sas:\n",
    "            TP += 1\n",
    "        elif pred_sa and pred_sa not in actual_las:\n",
    "            FP += 1\n",
    "            FP += 1\n",
    "            if actual_sas:\n",
    "                FP_WA += 1\n",
    "            else:\n",
    "                FP_NA += 1\n",
    "        elif not pred_sa and actual_sas:\n",
    "            FN += 1\n",
    "\n",
    "    details = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'FP_WA': FP_WA,\n",
    "        'FP_NA': FP_NA\n",
    "    }\n",
    "    \n",
    "    if TP == 0:\n",
    "        return 0, 0, 0, details\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "    return micro_f1, recall, precision, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search to Select the Best Parameters for Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/750 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/750 [00:00<05:53,  2.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/750 [00:00<06:04,  2.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/750 [00:01<06:11,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 4/750 [00:02<06:08,  2.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 5/750 [00:02<06:21,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 6/750 [00:03<06:24,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 7/750 [00:03<06:42,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 8/750 [00:04<06:32,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 9/750 [00:04<06:39,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|         | 10/750 [00:05<06:54,  1.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|         | 11/750 [00:05<07:02,  1.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 12/750 [00:06<07:06,  1.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 13/750 [00:07<06:51,  1.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 14/750 [00:07<06:44,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 15/750 [00:08<06:40,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 16/750 [00:08<06:39,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 17/750 [00:09<06:29,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|         | 18/750 [00:09<06:33,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 19/750 [00:10<06:38,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 20/750 [00:10<06:40,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 21/750 [00:11<06:49,  1.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 22/750 [00:12<08:12,  1.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 23/750 [00:12<07:59,  1.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 24/750 [00:13<07:32,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 25/750 [00:14<07:26,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|         | 26/750 [00:14<07:02,  1.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 27/750 [00:15<06:35,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 28/750 [00:15<06:35,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 29/750 [00:16<06:16,  1.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 30/750 [00:16<06:19,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 31/750 [00:17<06:23,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 32/750 [00:17<06:08,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|         | 33/750 [00:18<06:11,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 34/750 [00:18<06:08,  1.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 35/750 [00:19<05:59,  1.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 36/750 [00:19<06:11,  1.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 37/750 [00:20<06:03,  1.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 38/750 [00:20<06:24,  1.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 39/750 [00:21<06:25,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 40/750 [00:21<06:21,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|         | 41/750 [00:22<06:02,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 42/750 [00:22<06:01,  1.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 43/750 [00:23<05:57,  1.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 44/750 [00:23<05:52,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 45/750 [00:24<05:49,  2.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 46/750 [00:24<05:46,  2.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 47/750 [00:25<05:52,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|         | 48/750 [00:25<06:14,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 49/750 [00:26<06:12,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 50/750 [00:26<06:06,  1.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 51/750 [00:27<05:55,  1.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 52/750 [00:27<05:47,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 53/750 [00:28<05:46,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 54/750 [00:28<05:54,  1.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 55/750 [00:29<05:59,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|         | 56/750 [00:29<05:55,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 57/750 [00:30<05:45,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 58/750 [00:30<05:44,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 59/750 [00:31<05:39,  2.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 60/750 [00:31<05:36,  2.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 61/750 [00:32<06:56,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 62/750 [00:33<06:39,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|         | 63/750 [00:33<06:23,  1.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|         | 64/750 [00:34<06:16,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|         | 65/750 [00:34<06:06,  1.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|         | 66/750 [00:35<05:54,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|         | 67/750 [00:35<06:11,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|         | 68/750 [00:36<06:12,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-dc510a5f7cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtmp_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmicro_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_vals\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmicro_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-f2c4527483d2>\u001b[0m in \u001b[0;36mcompute_answers\u001b[0;34m(preds, candidates_dict, features, weights)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Converting logits to answer values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_token_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_tokens_probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_token_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_tokens_probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ans_type_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ans_type_probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \"\"\"\n\u001b[0;32m-> 1153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    default_range = np.arange(0, 1.05, 0.5)\n",
    "\n",
    "    weight_ranges = OrderedDict({\n",
    "            'ans_type_conf_weight': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            'start_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'end_pos_conf_weight': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "            'conf_bias': [-0.5, -0.25, 0, 0.25, 0.5],\n",
    "            'conf_threshold': [0.5]\n",
    "        })\n",
    "\n",
    "    combinations = list(itertools.product(*[weight_ranges[k] for k in weight_ranges.keys()]))\n",
    "\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "\n",
    "    results = {}\n",
    "    for weight_vals in tqdm.tqdm(combinations):\n",
    "        weights = {}\n",
    "        for weight_name, weight_val in zip(weight_ranges.keys(), weight_vals):\n",
    "                weights[weight_name] = weight_val\n",
    "\n",
    "        tmp_answers = compute_answers(preds, candidates_dict, eval_features, weights)\n",
    "        micro_f1, recall, precision, _ = score_preds(actual_answers, tmp_answers)\n",
    "        results[weight_vals] = (micro_f1, recall, precision)\n",
    "        \n",
    "    sr = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    print(sr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4668838219326819,\n",
       " 0.8739837398373984,\n",
       " 0.31851851851851853,\n",
       " {'TP': 215, 'FP': 460, 'FN': 31, 'FP_WA': 127, 'FP_NA': 189})"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_answers = compute_answers(preds, candidates_dict, eval_features, {\n",
    "        'ans_type_conf_weight': 0.4,\n",
    "        'start_pos_conf_weight': 0.3,\n",
    "        'end_pos_conf_weight': 0.3,\n",
    "        'conf_bias': 0,\n",
    "        'conf_threshold': 0.97\n",
    "    })\n",
    "score_preds(actual_answers, tmp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5584415584415585 0.41030534351145037 0.8739837398373984\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN = 215, 460 - int(189*.8), 31\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "micro_f1 = 2 * precision * recall / (precision + recall)\n",
    "print(micro_f1, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Micro-F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.46913580246913583\n",
      "Recall: 0.836\n",
      "Precision: 0.32605304212168484\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.test_post_processing:\n",
    "    actual_answers = get_actual_answers(FLAGS.train_file, FLAGS.n_examples)\n",
    "    micro_f1, recall, precision, _ = score_preds(actual_answers, answers)\n",
    "\n",
    "    print(f'Micro F1 Score: {micro_f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(answers):\n",
    "    submission_data = []\n",
    "\n",
    "    # Loop through answers in alphabetic order of example_ids\n",
    "    # This is how it's sorted in the sample submission\n",
    "    for example_id, answer in sorted(answers.items(), key=lambda x: x[0]):\n",
    "        long_answer_text = ''\n",
    "        if isinstance(answer['long_answer'], tuple):\n",
    "            long_answer_text = f'{answer[\"long_answer\"][0]}:{answer[\"long_answer\"][1]}'\n",
    "        else:\n",
    "            assert answer['long_answer'] is None, 'Invalid type of long answer!'\n",
    "            assert answer['short_answer'] is None, 'Cannot have a short answer with no long answer!'\n",
    "        long_answer_row = [f'{example_id}_long', long_answer_text]\n",
    "\n",
    "        short_answer_text = ''\n",
    "        if isinstance(answer['short_answer'], tuple):\n",
    "            short_answer_text = f'{answer[\"short_answer\"][0]}:{answer[\"short_answer\"][1]}'\n",
    "        elif answer['short_answer'] in ('YES', 'NO'):\n",
    "            short_answer_text = answer['short_answer']\n",
    "        else:\n",
    "            assert answer['short_answer'] is None, 'Invalid type of short answer!'\n",
    "        short_answer_row = [f'{example_id}_short', short_answer_text]\n",
    "\n",
    "        submission_data.append(long_answer_row)\n",
    "        submission_data.append(short_answer_row)\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data, columns=['example_id', 'PredictionString'])\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save the Submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   example_id PredictionString\n",
      "0   -9111510312671706854_long        2000:2187\n",
      "1  -9111510312671706854_short        2087:2095\n",
      "2   -9100123296297706673_long          519:620\n",
      "3  -9100123296297706673_short          598:601\n",
      "4   -9070556881023521969_long        2882:3126\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.test_post_processing:\n",
    "    submission_df = create_submission(answers)\n",
    "    print(submission_df.head())\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
