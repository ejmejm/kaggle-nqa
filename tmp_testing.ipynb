{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQA Utils Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import nqa_utils\n",
    "from scripts.nqa_utils import AnswerType\n",
    "from scripts import bert_modeling as modeling\n",
    "from scripts import bert_optimization\n",
    "from scripts import albert_optimization\n",
    "from scripts import albert\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Flags ###\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"albert\",\n",
    "    \"The name of model to use. Choose from ['bert', 'albert'].\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xxl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the ALBERT/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"data/albert_train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", -1,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_checkpoint_file\", \"tf2_albert_finetuned.ckpt\",\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_predictions_file\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"log_freq\", 128,\n",
    "    \"How many samples between each training log update.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"models/bert_joint_baseline/tf2_bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# This should be changed to 512 at some point,\n",
    "# as training was done with that value, it may\n",
    "# not make a big difference though\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 1, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 10000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "flags.DEFINE_string(\"tpu_name\", None, \"Name of the TPU to use.\")\n",
    "\n",
    "flags.DEFINE_string(\"tpu_zone\", None, \"Which zone the TPU is in.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "# TODO(Edan): Look at nested contents too at some point\n",
    "# Around 5% of long answers are nested, and around 50% of questions have\n",
    "# long answers\n",
    "# This means that this setting alone restricts us from a correct answer\n",
    "# around 2.5% of the time\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "## Custom flags\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", -1,\n",
    "    \"Number of examples to read from files. Only applicable during testing\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for training. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"data/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags\n",
    "\n",
    "VOCAB_SIZE = 30209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-0436dc814281>:9: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "# Training Records: 457137\n",
      "Changing the number of precomuted records listed to use all avaliable data.\n"
     ]
    }
   ],
   "source": [
    "def blocks(f, size=65536):\n",
    "    while True:\n",
    "        b = f.read(size)\n",
    "        if not b:\n",
    "            break\n",
    "        yield b\n",
    "\n",
    "n_records = 0\n",
    "for record in tf.compat.v1.python_io.tf_record_iterator(FLAGS.train_precomputed_file):\n",
    "    n_records += 1\n",
    "\n",
    "# with open(FLAGS.train_file, 'r') as f:\n",
    "#     n_train_examples = sum([bl.count('\\n') for bl in blocks(f)])\n",
    "\n",
    "# print('# Training Examples:', n_train_examples)\n",
    "print('# Training Records:', n_records)\n",
    "\n",
    "if FLAGS.do_train and FLAGS.train_num_precomputed != n_records:\n",
    "    print('Changing the number of precomuted records listed to use all avaliable data.')\n",
    "    FLAGS.train_num_precomputed = n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Generator for Training Data ###\n",
    "\n",
    "train_filenames = tf.io.gfile.glob(FLAGS.train_precomputed_file)\n",
    "\n",
    "name_to_features = {\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    output = ({\n",
    "        'input_ids': example['input_ids'],\n",
    "        'input_mask': example['input_mask'],\n",
    "        'segment_ids': example['segment_ids']\n",
    "    },\n",
    "    {\n",
    "        'tf_op_layer_start_logits': example['start_positions'],\n",
    "        'tf_op_layer_end_logits': example['end_positions'],\n",
    "        'ans_type_logits': example['answer_types']\n",
    "    })\n",
    "\n",
    "    return output\n",
    "\n",
    "def data_generator(batch_size=32, seed=42, valid_frac=0.05):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "\n",
    "    if valid_frac <= 0:\n",
    "        dataset = dataset.shuffle(buffer_size=20000, seed=seed, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "        dataset = dataset.repeat()\n",
    "        return dataset, None\n",
    "\n",
    "    train_size = int(FLAGS.train_num_precomputed * (1.0 - valid_frac))\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    valid_dataset = dataset.skip(train_size)\n",
    "\n",
    "    train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    valid_dataset = valid_dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "\n",
    "#     train_dataset = train_dataset.shuffle(buffer_size=20000, seed=seed, reshuffle_each_iteration=True)\n",
    "#     valid_dataset = valid_dataset.shuffle(buffer_size=5000, seed=seed, reshuffle_each_iteration=True)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "### Train the Model ###\n",
    "\n",
    "valid_frac = 0.02\n",
    "train_dataset, valid_dataset = data_generator(batch_size=2000, valid_frac=valid_frac)\n",
    "n_valid = np.ceil(FLAGS.train_num_precomputed * valid_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447994\n",
      "9142\n",
      "457137\n"
     ]
    }
   ],
   "source": [
    "print(int(FLAGS.train_num_precomputed * (1.0 - .02)))\n",
    "print(int(FLAGS.train_num_precomputed * .02))\n",
    "print(FLAGS.train_num_precomputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(ds)\n",
    "a = a[0]['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq = set([])\n",
    "\n",
    "for ids in a[:1000]:\n",
    "    curr_q = tuple(ids[:np.argmax(ids == 3)])\n",
    "    if not curr_q in uq:\n",
    "        uq.add(curr_q)\n",
    "#     print(curr_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206221\n"
     ]
    }
   ],
   "source": [
    "ds = iter(train_dataset)\n",
    "\n",
    "example_idx = 0\n",
    "uq = set([])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        a = next(ds)\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    a = a[0]['input_ids'].numpy()\n",
    "    for ids in a:\n",
    "        curr_q = tuple(ids[:np.argmax(ids == 3)])\n",
    "        if not curr_q in uq:\n",
    "            example_idx += 1\n",
    "            uq.add(curr_q)\n",
    "            \n",
    "print(len(uq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936\n"
     ]
    }
   ],
   "source": [
    "ds = iter(valid_dataset)\n",
    "\n",
    "example_idx = 0\n",
    "uq = set([])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        a = next(ds)\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    a = a[0]['input_ids'].numpy()\n",
    "    for ids in a:\n",
    "        curr_q = tuple(ids[:np.argmax(ids == 3)])\n",
    "        if not curr_q in uq:\n",
    "            example_idx += 1\n",
    "            uq.add(curr_q)\n",
    "            \n",
    "print(len(uq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = [13, 5, 836, 13, 6, 1488, 713, 13, 8, 400, 4436, 13, 8, 9427, 25983, 13, 8, 713, 13, 5, 64, 1322, 13, 15, 24305, 13, 6, 13246, 268, 3526, 1996, 22651, 12421, 34, 1119, 709, 34, 17119, 17, 18650, 30164, 30000, 201, 16, 4194, 25, 21, 1155, 4906, 5270, 1211, 34, 8683, 103, 25113, 467, 19, 1089, 13, 9, 14, 8798, 25, 15197, 9102, 7570, 242, 248, 13, 1, 4194, 13, 22, 22, 22030, 13, 15, 40, 909, 883, 37, 6162, 49, 23656, 72, 17707, 1549, 16, 23291, 17, 5713, 856, 37, 40, 274, 348, 13, 9, 24, 16309, 13, 21811, 509, 75, 21, 23808, 133, 20721, 27, 21, 22145, 19, 14, 1819, 2144, 29, 21, 6192, 5825, 377, 1098, 5197, 13, 9, 30165, 30001, 14, 1211, 63, 945, 91, 119, 652, 507, 3298, 3497, 13, 9, 32, 23, 4682, 34, 35, 639, 355, 479, 3107, 1947, 115, 142, 2217, 34, 680, 251, 7721, 836, 13, 15, 56, 467, 32, 19, 299, 1089, 13, 9, 14, 1063, 1322, 230, 14, 169, 19717, 1507, 26, 3209, 14, 249, 159, 13, 9, 32, 23, 67, 2519, 26, 16361, 603, 13, 22, 18, 836, 11137, 973, 13, 15, 113, 32, 23, 1571, 69, 34, 1314, 7477, 1358, 13, 9, 30166, 30002, 14, 484, 4064, 644, 22, 22164, 121, 4194, 23, 2519, 19, 14, 484, 16361, 615, 16, 14, 3013, 1009, 2535, 1746, 8828, 99, 18, 13, 15, 113, 32, 23, 1571, 69, 34, 8219, 1334, 1944, 13, 9, 14, 1211, 230, 14, 973, 1607, 18031, 1507, 13, 15, 21, 180, 1180, 1211, 450, 13, 9, 19, 894, 13, 15, 32, 230, 14, 2282, 13, 118, 1819, 189, 450, 26, 2233, 19, 246, 3035, 3209, 26, 122, 1089, 13, 8, 8, 973, 13, 9, 19, 563, 32, 23, 4876, 77, 21, 1580, 171, 1012, 34, 13, 1712, 1358, 29, 21, 8413, 34, 684, 12507, 62, 13, 9, 30167, 30003, 14, 1211, 2661, 29, 21, 1945, 37, 14, 1314, 13, 15, 56, 25, 40, 9315, 141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids_2 = [34, 680, 251, 7721, 836, 13, 15, 56, 467, 32, 19, 299, 1089, 13, 9, 14, 1063, 1322, 230, 14, 169, 19717, 1507, 26, 3209, 14, 249, 159, 13, 9, 32, 23, 67, 2519, 26, 16361, 603, 13, 22, 18, 836, 11137, 973, 13, 15, 113, 32, 23, 1571, 69, 34, 1314, 7477, 1358, 13, 9, 30166, 30002, 14, 484, 4064, 644, 22, 22164, 121, 4194, 23, 2519, 19, 14, 484, 16361, 615, 16, 14, 3013, 1009, 2535, 1746, 8828, 99, 18, 13, 15, 113, 32, 23, 1571, 69, 34, 8219, 1334, 1944, 13, 9, 14, 1211, 230, 14, 973, 1607, 18031, 1507, 13, 15, 21, 180, 1180, 1211, 450, 13, 9, 19, 894, 13, 15, 32, 230, 14, 2282, 13, 118, 1819, 189, 450, 26, 2233, 19, 246, 3035, 3209, 26, 122, 1089, 13, 8, 8, 973, 13, 9, 19, 563, 32, 23, 4876, 77, 21, 1580, 171, 1012, 34, 13, 1712, 1358, 29, 21, 8413, 34, 684, 12507, 62, 13, 9, 30167, 30003, 14, 1211, 2661, 29, 21, 1945, 37, 14, 1314, 13, 15, 56, 25, 40, 9315, 141, 16, 32, 13, 9, 15200, 13, 15, 14, 1945, 4359, 2894, 6079, 963, 13, 9, 32, 2589, 20, 4088, 17, 16525, 53, 16, 14, 360, 13, 22, 18, 407, 6876, 13, 45, 14, 23490, 16, 1770, 13, 9, 30168, 30004, 201, 16, 4194, 25, 17640, 77, 132, 4501, 13, 45, 30169, 30005, 19, 14, 64, 1050, 13, 15, 14, 407, 925, 13, 15, 34, 14, 204, 16, 15197, 9102, 22030, 13, 15, 40, 3035, 1155, 13, 15, 27704, 18, 88, 33, 4292, 19, 739, 13, 9, 33, 321, 258, 18, 21, 6655, 19, 6162, 49, 23656, 13, 9, 14, 23663, 1927, 14, 190, 29, 21, 3109, 24578, 8937, 17, 109, 3260, 16, 2383, 6182, 13, 9, 30170, 30006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(small, big):\n",
    "    for i in range(len(big)-len(small)+1):\n",
    "        for j in range(len(small)):\n",
    "            if big[i+j] != small[j]:\n",
    "                break\n",
    "        else:\n",
    "            return i, i+len(small)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "Found: 118294\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "Found: 376612\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n"
     ]
    }
   ],
   "source": [
    "ds = iter(train_dataset)\n",
    "\n",
    "b = 0\n",
    "while True:\n",
    "    try:\n",
    "        a = next(ds)\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    a = a[0]['input_ids'].numpy()\n",
    "    for i, ids in enumerate(a):\n",
    "        if contains(target_ids, ids):\n",
    "            print('Found:', b*2000 + i)\n",
    "    b += 1\n",
    "    \n",
    "    if b % 5 == 0:\n",
    "        print(b * 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "Found: 117050\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n"
     ]
    }
   ],
   "source": [
    "ds = iter(train_dataset)\n",
    "\n",
    "b = 0\n",
    "while True:\n",
    "    try:\n",
    "        a = next(ds)\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    a = a[0]['input_ids'].numpy()\n",
    "    for i, ids in enumerate(a):\n",
    "        if contains(target_ids_2, ids):\n",
    "            print('Found:', b*2000 + i)\n",
    "    b += 1\n",
    "    \n",
    "    if b % 5 == 0:\n",
    "        print(b * 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "ds = iter(valid_dataset)\n",
    "\n",
    "b = 0\n",
    "while True:\n",
    "    try:\n",
    "        a = next(ds)\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    a = a[0]['input_ids'].numpy()\n",
    "    for i, ids in enumerate(a):\n",
    "        if contains(target_ids_2, ids):\n",
    "            print('Found:', b*2000 + i)\n",
    "    b += 1\n",
    "    \n",
    "    if b % 5 == 0:\n",
    "        print(b * 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = iter(train_dataset)\n",
    "for i in range(188):\n",
    "    next(ds)\n",
    "\n",
    "data = next(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 182)\n",
      "tf.Tensor(\n",
      "[    2 30204    98    25    14  5825    19   201    16  4194     3 30150\n",
      " 30207 30151 30050   201    16  4194   201    16  4194  1227  1314  8683\n",
      "   103 25113   501   581   201    16  4194   475   836   816   486  5975\n",
      "  3209  5916   680   251  7721   836  3176  1231   299   547    13    15\n",
      "  1089    13     5   836    13     6  1488   713    13     8   400  4436\n",
      "    13     8  9427 25983    13     8   713    13     5    64  1322    13\n",
      "    15 24305    13     6 13246   268  3526  1996 22651 12421    34  1119\n",
      "   709    34 17119    17 18650 30164 30000   201    16  4194    25    21\n",
      "  1155  4906  5270  1211    34  8683   103 25113   467    19  1089    13\n",
      "     9    14  8798    25 15197  9102  7570   242   248    13     1  4194\n",
      "    13    22    22 22030    13    15    40   909   883    37  6162    49\n",
      " 23656    72 17707  1549    16 23291    17  5713   856    37    40   274\n",
      "   348    13     9    24 16309    13 21811   509    75    21 23808   133\n",
      " 20721    27    21 22145    19    14  1819  2144    29    21  6192  5825\n",
      "   377  1098  5197    13     9 30165 30001    14  1211    63   945    91\n",
      "   119   652   507  3298  3497    13     9    32    23  4682    34    35\n",
      "   639   355   479  3107  1947   115   142  2217    34   680   251  7721\n",
      "   836    13    15    56   467    32    19   299  1089    13     9    14\n",
      "  1063  1322   230    14   169 19717  1507    26  3209    14   249   159\n",
      "    13     9    32    23    67  2519    26 16361   603    13    22    18\n",
      "   836 11137   973    13    15   113    32    23  1571    69    34  1314\n",
      "  7477  1358    13     9 30166 30002    14   484  4064   644    22 22164\n",
      "   121  4194    23  2519    19    14   484 16361   615    16    14  3013\n",
      "  1009  2535  1746  8828    99    18    13    15   113    32    23  1571\n",
      "    69    34  8219  1334  1944    13     9    14  1211   230    14   973\n",
      "  1607 18031  1507    13    15    21   180  1180  1211   450    13     9\n",
      "    19   894    13    15    32   230    14  2282    13   118  1819   189\n",
      "   450    26  2233    19   246  3035  3209    26   122  1089    13     8\n",
      "     8   973    13     9    19   563    32    23  4876    77    21  1580\n",
      "   171  1012    34    13  1712  1358    29    21  8413    34   684 12507\n",
      "    62    13     9 30167 30003    14  1211  2661    29    21  1945    37\n",
      "    14  1314    13    15    56    25    40  9315   141    16    32     3], shape=(384,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "nd = {k: x[612] for k, x in data[0].items()}\n",
    "print(contains(target_ids, nd['input_ids']))\n",
    "print(nd['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
