{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Q&A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "/kaggle/input/tensorflow2-question-answering/sample_submission.csv\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\n",
      "/kaggle/input/bertjointbaseline/bert_joint.ckpt.data-00000-of-00001\n",
      "/kaggle/input/bertjointbaseline/vocab-nq.txt\n",
      "/kaggle/input/bertjointbaseline/bert_config.json\n",
      "/kaggle/input/bertjointbaseline/nq-train.tfrecords-00000-of-00001\n",
      "/kaggle/input/bertjointbaseline/bert_joint.ckpt.index\n",
      "/kaggle/input/bert-q-a-training/__results__.html\n",
      "/kaggle/input/bert-q-a-training/custom.css\n",
      "/kaggle/input/bert-q-a-training/__notebook__.ipynb\n",
      "/kaggle/input/bert-q-a-training/__output__.json\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/__results__.html\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/custom.css\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/__notebook__.ipynb\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/tf2_bert_joint.ckpt.data-00000-of-00001\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/tf2_bert_joint.ckpt.index\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/__output__.json\n",
      "/kaggle/input/bert-q-a-convert-weights-to-2-0/checkpoint\n",
      "/kaggle/input/qa-train-record/train.tf_record\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# import tf2_0_baseline_w_bert as tf2baseline # old script\n",
    "import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "import bert_modeling as modeling\n",
    "import bert_optimization as optimization\n",
    "import bert_tokenization as tokenization\n",
    "import tqdm\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", \"/kaggle/input/bertjointbaseline/bert_config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"/kaggle/input/bertjointbaseline/vocab-nq.txt\",\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"/kaggle/working/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"/kaggle/input/qa-train-record/train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", 6000,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_prediction_file\", \"tf2_bert_finetuned.ckpt\",\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_ckeckpoint\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"/kaggle/working/logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"/kaggle/input/bert-q-a-convert-weights-to-2-0/tf2_bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# This should be changed to 512 at some point,\n",
    "# as training was done with that value, it may\n",
    "# not make a big difference though\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 2, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "## Custom flags\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", -1,\n",
    "    \"Number of examples to read from files. Only applicable during testing\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for training. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Number of Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # https://stackoverflow.com/questions/9629179/python-counting-lines-in-a-huge-10gb-file-as-fast-as-possible\n",
    "    def blocks(f, size=65536):\n",
    "      while True:\n",
    "        b = f.read(size)\n",
    "        if not b:\n",
    "          break\n",
    "        yield b\n",
    "\n",
    "    n_records = 0\n",
    "    for record in tf.compat.v1.python_io.tf_record_iterator(FLAGS.train_precomputed_file):\n",
    "      n_records += 1\n",
    "\n",
    "    with open(FLAGS.train_file, 'r') as f:\n",
    "      n_train_examples = sum([bl.count('\\n') for bl in blocks(f)])\n",
    "\n",
    "    print('# Training Examples:', n_train_examples)\n",
    "    print('# Training Records:', n_records)\n",
    "\n",
    "    if FLAGS.do_train:\n",
    "      assert FLAGS.train_num_precomputed == n_records, \\\n",
    "        'Number of records does not match up with the given records file!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "          raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                          \"non-floating point (and non-complex) \"\n",
    "                          \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "          raise ValueError(\"The last dimension of the inputs to \"\n",
    "                           \"`TDense` should be defined. \"\n",
    "                           \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
    "\n",
    "def build_model(bert_config):\n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "    bert_layer = modeling.BertModel(config=bert_config, name='bert')\n",
    "    pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n",
    "                                                input_mask=input_mask,\n",
    "                                                input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    return tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='bert_baseline')\n",
    "\n",
    "\n",
    "def compile_model(model, learning_rate,\n",
    "                  num_train_steps, num_warmup_steps,\n",
    "                  init_checkpoint=None):\n",
    "    \n",
    "    if init_checkpoint:\n",
    "        model.load_weights(init_checkpoint)\n",
    "        print('Loaded model weights!')\n",
    "    \n",
    "    # TODO(Edan): Add a way to have no loss on this for when there is no answer\n",
    "    # Computes the loss for positions.\n",
    "    def compute_loss(positions, logits):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            tf.cast(positions, tf.int32), depth=FLAGS.max_seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            input_tensor=tf.reduce_sum(input_tensor=one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "    # Computes the loss for labels.\n",
    "    def compute_label_loss(labels, logits):\n",
    "        one_hot_labels = tf.one_hot(\n",
    "            tf.cast(labels, tf.int32), depth=len(tf2baseline.AnswerType), dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            input_tensor=tf.reduce_sum(input_tensor=one_hot_labels * log_probs, axis=-1))\n",
    "        return loss\n",
    "    \n",
    "    losses = {\n",
    "        'tf_op_layer_start_logits': compute_loss,\n",
    "        'tf_op_layer_end_logits': compute_loss,\n",
    "        'ans_type_logits': compute_label_loss\n",
    "    }\n",
    "    loss_weights = {\n",
    "        'tf_op_layer_start_logits': 1.0,\n",
    "        'tf_op_layer_end_logits': 1.0,\n",
    "        'ans_type_logits': 1.0\n",
    "    }\n",
    "\n",
    "    optimizer = optimization.create_optimizer(learning_rate,\n",
    "                                              num_train_steps,\n",
    "                                              num_warmup_steps)\n",
    "    \n",
    "    comp_model = model.compile(optimizer=optimizer,\n",
    "                               loss=losses,\n",
    "                               loss_weights=loss_weights,\n",
    "                               metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_baseline\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "td_seq (TDense)                 (None, 384, 2)       2050        bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 384, 1), (No 0           td_seq[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_logits (Tenso [(None, 384)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_logits (TensorF [(None, 384)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type_logits (TDense)        (None, 5)            5125        bert[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 335,149,063\n",
      "Trainable params: 335,149,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "model = build_model(bert_config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Pretrained Model Weights to Tensorflow 2.0 (Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Map var names to vars\n",
    "    model_params = {v.name:v for v in model.trainable_variables}\n",
    "    # Get all root name paths to wieghts\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    print(model_roots)\n",
    "    # Get all saves var names\n",
    "    saved_names = [k for k,v in tf.train.list_variables('../input/bertjointbaseline/bert_joint.ckpt')]\n",
    "    a_map = {v:v+':0' for v in saved_names}\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    def transform(x):\n",
    "        x = x.replace('attention/self','attention')\n",
    "        x = x.replace('attention','self_attention')\n",
    "        x = x.replace('attention/output','attention_output')  \n",
    "\n",
    "        x = x.replace('/dense','')\n",
    "        x = x.replace('/LayerNorm','_layer_norm')\n",
    "        x = x.replace('embeddings_layer_norm','embeddings/layer_norm')  \n",
    "\n",
    "        x = x.replace('attention_output_layer_norm','attention_layer_norm')  \n",
    "        x = x.replace('embeddings/word_embeddings','word_embeddings/embeddings')\n",
    "\n",
    "        x = x.replace('/embeddings/','/embedding_postprocessor/')  \n",
    "        x = x.replace('/token_type_embeddings','/type_embeddings')  \n",
    "        x = x.replace('/pooler/','/pooler_transform/')  \n",
    "        x = x.replace('answer_type_output_bias','ans_type_logits/bias')  \n",
    "        x = x.replace('answer_type_output_','ans_type_logits/')\n",
    "        x = x.replace('cls/nq/output_','td_seq/')\n",
    "        x = x.replace('/weights','/kernel')\n",
    "\n",
    "        return x\n",
    "    # Maps saved name to new name\n",
    "    a_map = {k:model_params.get(transform(v),None) for k,v in a_map.items() if k!='global_step'}\n",
    "    for saved_name, curr_var in a_map.items():\n",
    "        if curr_var is None:\n",
    "            print(saved_name)\n",
    "    print('---------------')\n",
    "    print('Missaligned variables:', set([p for p in model_params.keys()]).difference(\n",
    "        set([v.name if v is not None else None for v in a_map.values()])))\n",
    "    tf.compat.v1.train.init_from_checkpoint(ckpt_dir_or_file='../input/bertjointbaseline/bert_joint.ckpt',\n",
    "                                            assignment_map=a_map)\n",
    "    \n",
    "    model.save_weights('/kaggle/working/tf2_bert_joint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights!\n"
     ]
    }
   ],
   "source": [
    "num_train_features = FLAGS.train_num_precomputed\n",
    "num_train_steps = int(num_train_features / FLAGS.train_batch_size *\n",
    "                      FLAGS.num_train_epochs)\n",
    "\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "compile_model(model=model,\n",
    "              learning_rate=FLAGS.learning_rate,\n",
    "              num_train_steps=num_train_steps,\n",
    "              num_warmup_steps=num_warmup_steps,\n",
    "              init_checkpoint=FLAGS.init_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Formatted Training Data (TFRecord, Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def data_generator(path, chunk_size=30000):\n",
    "        curr_pos = 0\n",
    "        last_line = False\n",
    "        with open(path, 'rt') as f:\n",
    "            while not last_line:\n",
    "                examples = []\n",
    "                for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                    line = f.readline()\n",
    "                    if line is None:\n",
    "                        last_line = True\n",
    "                        break\n",
    "                    examples.append(tf2baseline.create_example_from_jsonl(line))\n",
    "                    examples[-1] = tf2baseline.read_nq_entry(examples[-1], FLAGS.do_train)[0]\n",
    "                curr_pos = i + 1\n",
    "                yield examples\n",
    "            \n",
    "    chunk_size = 10\n",
    "    example_gen = data_generator(FLAGS.train_file, chunk_size=chunk_size)\n",
    "\n",
    "    train_writer = tf2baseline.FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "        is_training=FLAGS.do_train)\n",
    "    train_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        train_features.append(feature)\n",
    "        train_writer.process_feature(feature)\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    for _ in tqdm.tqdm(range(int(np.ceil(n_train_examples/chunk_size)))):\n",
    "        # If we want to change how the features are generated then this\n",
    "        # is the function\n",
    "        # Right now if an answer is split over 2 blocks, I think the\n",
    "        # end pos of the first is the last token, and the first pos of\n",
    "        # the second is the first token of the respective blocks\n",
    "        num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "            examples=next(example_gen),\n",
    "            tokenizer=tokenizer,\n",
    "            is_training=FLAGS.do_train,\n",
    "            output_fn=append_feature)\n",
    "\n",
    "        train_writer._writer.flush()\n",
    "\n",
    "    train_writer.close()\n",
    "    train_filename = train_writer.filename\n",
    "\n",
    "    print(f'# Features written: {train_writer.num_features}\\n')\n",
    "\n",
    "    print('**Features**\\n')\n",
    "\n",
    "    for e in dir(train_features[0]):\n",
    "        if not e.startswith('__'):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = tf.io.gfile.glob(FLAGS.train_precomputed_file)\n",
    "\n",
    "name_to_features = {\n",
    "    \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def data_generator(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    if 'seed' not in params:\n",
    "        params['seed'] = 42\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "    if FLAGS.do_train:\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=5000, seed=params['seed'])\n",
    "        \n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    \n",
    "    data_iter = iter(dataset)\n",
    "    for examples in data_iter:\n",
    "        inputs = {\n",
    "            # 'unique_id': examples['unique_ids'],\n",
    "            'input_ids': examples['input_ids'],\n",
    "            'input_mask': examples['input_mask'],\n",
    "            'segment_ids': examples['segment_ids']\n",
    "        }\n",
    "\n",
    "        targets = {\n",
    "            'tf_op_layer_start_logits': examples['start_positions'],\n",
    "            'tf_op_layer_end_logits': examples['end_positions'],\n",
    "            'ans_type_logits': examples['answer_types'],\n",
    "        }\n",
    "\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training callbacks\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(FLAGS.output_dir, FLAGS.output_prediction_file), monitor='val_acc', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='max', period=1)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=FLAGS.log_dir, update_freq=128)\n",
    "\n",
    "# Change this to True to actually train\n",
    "if False:\n",
    "    H = model.fit_generator(data_generator({'batch_size': FLAGS.train_batch_size}),\n",
    "                            steps_per_epoch=FLAGS.train_num_precomputed // FLAGS.train_batch_size,\n",
    "                            epochs=FLAGS.num_train_epochs,\n",
    "                            callbacks=[ckpt_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download for Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/logs/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8750708e8bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Unzip and run using \"tensorboard --logdir=\"./logs\" --port=6006\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs.tar.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w:gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, name, arcname, recursive, exclude, filter)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         \u001b[0;31m# Create a TarInfo object from the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1934\u001b[0;31m         \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettarinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mgettarinfo\u001b[0;34m(self, name, arcname, fileobj)\u001b[0m\n\u001b[1;32m   1801\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstat\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdereference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m                 \u001b[0mstatres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                 \u001b[0mstatres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/logs/'"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Unzip and run using \"tensorboard --logdir=\"./logs\" --port=6006\"\n",
    "tar = tarfile.open(\"logs.tar.gz\", \"w:gz\")\n",
    "tar.add(FLAGS.log_dir, arcname=\"logs\")\n",
    "tar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
