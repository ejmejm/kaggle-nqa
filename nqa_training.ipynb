{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Q&A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.0 Baseline Loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
    "from scripts.tf2_0_baseline_w_bert_translated_to_tf2_0 import AnswerType\n",
    "from scripts import bert_modeling as modeling\n",
    "from scripts import bert_optimization as optimization\n",
    "from scripts import bert_tokenization as tokenization\n",
    "from scripts import albert\n",
    "from scripts import albert_tokenization\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import absl\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(absl.flags.FLAGS)\n",
    "\n",
    "flags = absl.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"albert\",\n",
    "    \"The name of model to use. Choose from ['bert', 'albert'].\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"config_file\", \"models/albert_xxl/config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT/ALBERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"models/albert_xxl/vocab/modified-30k-clean.model\",\n",
    "                    \"The vocabulary file that the ALBERT/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"output/\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", \"data/train.tf_record\",\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", 272565,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_checkpoint_file\", \"tf2_albert_finetuned.ckpt\",\n",
    "    \"Where to save finetuned checkpoints to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_predictions_file\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\", \"logs/\",\n",
    "    \"Where logs, specifically Tensorboard logs, will be saved to.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_init_checkpoint\", \"models/bert_joint_baseline/tf2_bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# This should be changed to 512 at some point,\n",
    "# as training was done with that value, it may\n",
    "# not make a big difference though\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 1, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_epochs\", 3,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
    "\n",
    "absl.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "# TODO(Edan): Look at nested contents too at some point\n",
    "# Around 5% of long answers are nested, and around 50% of questions have\n",
    "# long answers\n",
    "# This means that this setting alone restricts us from a correct answer\n",
    "# around 2.5% of the time\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "## Custom flags\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_examples\", -1,\n",
    "    \"Number of examples to read from files. Only applicable during testing\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"train_file\", \"data/simplified-nq-train.jsonl\",\n",
    "    \"NQ json for training. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"albert_pretrain_checkpoint\", \"models/albert_xxl/tf2_model.h5\",\n",
    "    \"Pretrain checkpoint (for Albert only).\")\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"data/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv) # Parse the flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Number of Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # https://stackoverflow.com/questions/9629179/python-counting-lines-in-a-huge-10gb-file-as-fast-as-possible\n",
    "    def blocks(f, size=65536):\n",
    "        while True:\n",
    "            b = f.read(size)\n",
    "            if not b:\n",
    "                break\n",
    "            yield b\n",
    "\n",
    "    n_records = 0\n",
    "    for record in tf.compat.v1.python_io.tf_record_iterator(FLAGS.train_precomputed_file):\n",
    "        n_records += 1\n",
    "\n",
    "    with open(FLAGS.train_file, 'r') as f:\n",
    "        n_train_examples = sum([bl.count('\\n') for bl in blocks(f)])\n",
    " \n",
    "    print('# Training Examples:', n_train_examples)\n",
    "    print('# Training Records:', n_records)\n",
    "\n",
    "    if FLAGS.do_train:\n",
    "        assert FLAGS.train_num_precomputed == n_records, \\\n",
    "            'Number of records does not match up with the given records file!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                            \"non-floating point (and non-complex) \"\n",
    "                            \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                             \"`TDense` should be defined. \"\n",
    "                             \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_model(config_file):\n",
    "    \"\"\"Builds and returns a BERT model.\"\"\"\n",
    "    config = modeling.BertConfig.from_json_file(config_file)\n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    bert_layer = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = bert_layer(input_word_ids=input_ids,\n",
    "                                                input_mask=input_mask,\n",
    "                                                input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    return tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='bert_baseline')\n",
    "\n",
    "def get_albert_model(config_file):\n",
    "    \"\"\"Create an Albert model from pretrained configuration file with vocab_size changed\n",
    "    to 30522, and optionally loads the pretrained weights.\n",
    "    \"\"\"\n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    config.vocab_size = 30522\n",
    "    albert_layer = albert.AlbertModel(config=config)\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "    pooled_output, sequence_output = albert_layer(input_word_ids=input_ids,\n",
    "                                                    input_mask=input_mask,\n",
    "                                                    input_type_ids=segment_ids)\n",
    "    \n",
    "    # Maybe try sharing the start and end logits variables\n",
    "    seq_layer = TDense(2, name='td_seq')\n",
    "    # seq_layer = tf.keras.layers.TimeDistributed(seq_layer, name='td_seq')\n",
    "    \n",
    "    seq_logits = seq_layer(sequence_output)\n",
    "    start_logits, end_logits = tf.split(seq_logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_logits')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_logits')\n",
    "    \n",
    "    ans_type_layer = TDense(len(tf2baseline.AnswerType), name='ans_type_logits')\n",
    "    ans_type_logits = ans_type_layer(pooled_output)\n",
    "    \n",
    "    albert_model = tf.keras.Model([input_ids, input_mask, segment_ids],\n",
    "                          [start_logits, end_logits, ans_type_logits],\n",
    "                          name='albert')\n",
    "        \n",
    "    return albert_model\n",
    "\n",
    "def build_model(model_name, config_file):\n",
    "    \"\"\"Build model according to model_name.\n",
    "    \n",
    "    Args:\n",
    "        model_name: ['bert', 'albert']\n",
    "        config_file: path to config file\n",
    "        pretrain_ckpt: path to pretrain checkpoint (albert only)\n",
    "    Returns:\n",
    "        the specified model\n",
    "    \"\"\"\n",
    "    if model_name == 'albert':\n",
    "        model = get_albert_model(config_file)\n",
    "    elif model_name == 'bert':\n",
    "        model = get_bert_model(config_file)\n",
    "    else:\n",
    "        raise ValueError('{} is not supported'.format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all pretrain weights of albert\n",
    "def freeze_pretrain_weights(model):\n",
    "    \"\"\"Freeze pretrain weights of the albert model.\n",
    "    \"\"\"\n",
    "    albert_layer = model.get_layer('albert_model')\n",
    "    albert_layer.embedding_postprocessor.trainable = False\n",
    "    albert_layer.encoder.trainable = False\n",
    "    albert_layer.pooler_transform.trainable = False\n",
    "\n",
    "# helper function to load the pretrain weights\n",
    "def load_pretrain_weights(model, config_file, ckpt_file):\n",
    "    \"\"\"Loads the pretrained model's weights, except for the embedding layer,\n",
    "    into the new model, which has embedding vocab size of 30522 instead of 30000.\n",
    "    \n",
    "    Args:\n",
    "        model: the same model architecture as the pre-trained model except for embedding\n",
    "        config_file: path to the config file to re-create the pre-trained model\n",
    "        ckpt_file: path to the checkpoint of the pre-trained model\n",
    "    \"\"\"\n",
    "    # re-create the pre-trained model\n",
    "    config = albert.AlbertConfig.from_json_file(config_file)\n",
    "    albert_layer_pretrain = albert.AlbertModel(config=config, name='albert_pretrain')\n",
    "\n",
    "    input_ids_pretrain = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_ids_pretrain')\n",
    "    input_mask_pretrain = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='input_mask_pretrain')\n",
    "    segment_ids_pretrain = tf.keras.Input(shape=(FLAGS.max_seq_length,),dtype=tf.int32,name='segment_ids_pretrain')\n",
    "\n",
    "    pooled_output_pretrain, sequence_output_pretrain = albert_layer_pretrain(input_word_ids=input_ids_pretrain,\n",
    "                                                    input_mask=input_mask_pretrain,\n",
    "                                                    input_type_ids=segment_ids_pretrain)\n",
    "\n",
    "    albert_model_pretrain = tf.keras.Model(inputs=[input_ids_pretrain,input_mask_pretrain,segment_ids_pretrain], \n",
    "           outputs=[pooled_output_pretrain, sequence_output_pretrain])\n",
    "    \n",
    "    # load the weights into the pre-trained model\n",
    "    albert_model_pretrain.load_weights(ckpt_file)\n",
    "    \n",
    "    # set the pre-train weights on the new model\n",
    "    albert_layer = model.get_layer('albert_model')\n",
    "    albert_layer.embedding_postprocessor.set_weights(albert_layer_pretrain.embedding_postprocessor.get_weights())\n",
    "    albert_layer.encoder.set_weights(albert_layer_pretrain.encoder.get_weights())\n",
    "    albert_layer.pooler_transform.set_weights(albert_layer_pretrain.pooler_transform.get_weights())\n",
    "    \n",
    "    del albert_model_pretrain\n",
    "    \n",
    "def compile_model(model, model_type, learning_rate,\n",
    "                  num_train_steps, num_warmup_steps,\n",
    "                  init_checkpoint=None):\n",
    "    \n",
    "    if model_type.lower() not in ('bert', 'albert'):\n",
    "        raise ValueError('`model_type` must be one of the following values: [\"bert\", \"albert\"]!')\n",
    "    \n",
    "    if init_checkpoint:\n",
    "        if model_type.lower() == 'bert':\n",
    "            model.load_weights(init_checkpoint)\n",
    "            print('Loaded model weights!')\n",
    "        elif model_type.lower() == 'albert':\n",
    "            load_pretrain_weights(model, FLAGS.config_file, init_checkpoint)\n",
    "    \n",
    "    # TODO(Edan): Add a way to have no loss on this for when there is no answer\n",
    "    # Computes the loss for positions.\n",
    "    def compute_loss(positions, logits):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            tf.cast(positions, tf.int32), depth=FLAGS.max_seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            input_tensor=tf.reduce_sum(input_tensor=one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "    # Computes the loss for labels.\n",
    "    def compute_label_loss(labels, logits):\n",
    "        one_hot_labels = tf.one_hot(\n",
    "            tf.cast(labels, tf.int32), depth=len(tf2baseline.AnswerType), dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            input_tensor=tf.reduce_sum(input_tensor=one_hot_labels * log_probs, axis=-1))\n",
    "        return loss\n",
    "    \n",
    "    losses = {\n",
    "        'tf_op_layer_start_logits': compute_loss,\n",
    "        'tf_op_layer_end_logits': compute_loss,\n",
    "        'ans_type_logits': compute_label_loss\n",
    "    }\n",
    "    loss_weights = {\n",
    "        'tf_op_layer_start_logits': 1.0,\n",
    "        'tf_op_layer_end_logits': 1.0,\n",
    "        'ans_type_logits': 1.0\n",
    "    }\n",
    "\n",
    "    optimizer = optimization.create_optimizer(learning_rate,\n",
    "                                              num_train_steps,\n",
    "                                              num_warmup_steps)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=losses,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"albert\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "albert_model (AlbertModel)      ((None, 2048), (None 58791680    input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "td_seq (TDense)                 (None, 384, 2)       4098        albert_model[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 384, 1), (No 0           td_seq[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_logits (Tenso [(None, 384)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_logits (TensorF [(None, 384)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type_logits (TDense)        (None, 5)            10245       albert_model[0][0]               \n",
      "==================================================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 113,690,887\n",
      "Trainable params: 58,806,023\n",
      "Non-trainable params: 54,884,864\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "num_train_features = FLAGS.train_num_precomputed\n",
    "num_train_steps = int(num_train_features / FLAGS.train_batch_size *\n",
    "                      FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "model = build_model(FLAGS.model, FLAGS.config_file)\n",
    "model.load_weights('models/albert_xl/tmp_albert_weights.h5')\n",
    "compile_model(model=model,\n",
    "              model_type=FLAGS.model,\n",
    "              learning_rate=FLAGS.learning_rate,\n",
    "              num_train_steps=num_train_steps,\n",
    "              num_warmup_steps=num_warmup_steps,\n",
    "              init_checkpoint=None)\n",
    "# compile_model(model=model,\n",
    "#               model_type=FLAGS.model,\n",
    "#               learning_rate=FLAGS.learning_rate,\n",
    "#               num_train_steps=num_train_steps,\n",
    "#               num_warmup_steps=num_warmup_steps,\n",
    "#               init_checkpoint=FLAGS.albert_pretrain_checkpoint)\n",
    "\n",
    "if FLAGS.model.lower() == 'albert':\n",
    "    freeze_pretrain_weights(model)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Pretrained Model Weights to Tensorflow 2.0 (Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Map var names to vars\n",
    "    model_params = {v.name:v for v in model.trainable_variables}\n",
    "    # Get all root name paths to wieghts\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    print(model_roots)\n",
    "    # Get all saves var names\n",
    "    saved_names = [k for k,v in tf.train.list_variables('../input/bertjointbaseline/bert_joint.ckpt')]\n",
    "    a_map = {v:v+':0' for v in saved_names}\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    def transform(x):\n",
    "        x = x.replace('attention/self','attention')\n",
    "        x = x.replace('attention','self_attention')\n",
    "        x = x.replace('attention/output','attention_output')  \n",
    "\n",
    "        x = x.replace('/dense','')\n",
    "        x = x.replace('/LayerNorm','_layer_norm')\n",
    "        x = x.replace('embeddings_layer_norm','embeddings/layer_norm')  \n",
    "\n",
    "        x = x.replace('attention_output_layer_norm','attention_layer_norm')  \n",
    "        x = x.replace('embeddings/word_embeddings','word_embeddings/embeddings')\n",
    "\n",
    "        x = x.replace('/embeddings/','/embedding_postprocessor/')  \n",
    "        x = x.replace('/token_type_embeddings','/type_embeddings')  \n",
    "        x = x.replace('/pooler/','/pooler_transform/')  \n",
    "        x = x.replace('answer_type_output_bias','ans_type_logits/bias')  \n",
    "        x = x.replace('answer_type_output_','ans_type_logits/')\n",
    "        x = x.replace('cls/nq/output_','td_seq/')\n",
    "        x = x.replace('/weights','/kernel')\n",
    "\n",
    "        return x\n",
    "    # Maps saved name to new name\n",
    "    a_map = {k:model_params.get(transform(v),None) for k,v in a_map.items() if k!='global_step'}\n",
    "    for saved_name, curr_var in a_map.items():\n",
    "        if curr_var is None:\n",
    "            print(saved_name)\n",
    "    print('---------------')\n",
    "    print('Missaligned variables:', set([p for p in model_params.keys()]).difference(\n",
    "        set([v.name if v is not None else None for v in a_map.values()])))\n",
    "    tf.compat.v1.train.init_from_checkpoint(ckpt_dir_or_file='../input/bertjointbaseline/bert_joint.ckpt',\n",
    "                                            assignment_map=a_map)\n",
    "    \n",
    "    model.save_weights('/kaggle/working/tf2_bert_joint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Formatted Training Data (TFRecord, Only Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def data_generator(path, chunk_size=30000):\n",
    "        curr_pos = 0\n",
    "        last_line = False\n",
    "        with open(path, 'rt') as f:\n",
    "            while not last_line:\n",
    "                examples = []\n",
    "                for i in range(curr_pos, curr_pos+chunk_size):\n",
    "                    line = f.readline()\n",
    "                    if line is None:\n",
    "                        last_line = True\n",
    "                        break\n",
    "                    examples.append(tf2baseline.create_example_from_jsonl(line))\n",
    "                    examples[-1] = tf2baseline.read_nq_entry(examples[-1], FLAGS.do_train)[0]\n",
    "                curr_pos = i + 1\n",
    "                yield examples\n",
    "            \n",
    "    chunk_size = 100\n",
    "    example_gen = data_generator(FLAGS.train_file, chunk_size=chunk_size)\n",
    "\n",
    "    train_writer = tf2baseline.FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "        is_training=FLAGS.do_train)\n",
    "    train_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        train_features.append(feature)\n",
    "        train_writer.process_feature(feature)\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    for _ in tqdm.tqdm(range(int(np.ceil(n_train_examples/chunk_size)))):\n",
    "        # If we want to change how the features are generated then this\n",
    "        # is the function\n",
    "        # Right now if an answer is split over 2 blocks, I think the\n",
    "        # end pos of the first is the last token, and the first pos of\n",
    "        # the second is the first token of the respective blocks\n",
    "        num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "            examples=next(example_gen),\n",
    "            tokenizer=tokenizer,\n",
    "            is_training=FLAGS.do_train,\n",
    "            output_fn=append_feature)\n",
    "\n",
    "        train_writer._writer.flush()\n",
    "\n",
    "    train_writer.close()\n",
    "    train_filename = train_writer.filename\n",
    "\n",
    "    print(f'# Features written: {train_writer.num_features}\\n')\n",
    "\n",
    "    print('**Features**\\n')\n",
    "\n",
    "    for e in dir(train_features[0]):\n",
    "        if not e.startswith('__'):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = tf.io.gfile.glob(FLAGS.train_precomputed_file)\n",
    "\n",
    "name_to_features = {\n",
    "    \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([FLAGS.max_seq_length], tf.int64),\n",
    "}\n",
    "if FLAGS.do_train:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"answer_types\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def data_generator(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    if 'seed' not in params:\n",
    "        params['seed'] = 42\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "    if FLAGS.do_train:\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=5000, seed=params['seed'])\n",
    "        \n",
    "    dataset = dataset.map(lambda r: decode_record(r, name_to_features))\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "    \n",
    "    data_iter = iter(dataset)\n",
    "    for examples in data_iter:\n",
    "        inputs = {\n",
    "            # 'unique_id': examples['unique_ids'],\n",
    "            'input_ids': examples['input_ids'],\n",
    "            'input_mask': examples['input_mask'],\n",
    "            'segment_ids': examples['segment_ids']\n",
    "        }\n",
    "\n",
    "        targets = {\n",
    "            'tf_op_layer_start_logits': examples['start_positions'],\n",
    "            'tf_op_layer_end_logits': examples['end_positions'],\n",
    "            'ans_type_logits': examples['answer_types'],\n",
    "        }\n",
    "\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "Epoch 1/3\n",
      " 41349/272565 [===>..........................] - ETA: 54:40:46 - loss: 13.1110 - tf_op_layer_start_logits_loss: 5.9539 - tf_op_layer_end_logits_loss: 5.9546 - ans_type_logits_loss: 1.2058 - tf_op_layer_start_logits_sparse_categorical_accuracy: 0.0028 - tf_op_layer_end_logits_sparse_categorical_accuracy: 0.0023 - ans_type_logits_sparse_categorical_accuracy: 0.7939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-068d6402ca80>\", line 14, in <module>\n",
      "    callbacks=[ckpt_callback, tensorboard_callback])\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1297, in fit_generator\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 265, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 973, in train_on_batch\n",
      "    class_weight=class_weight, reset_metrics=reset_metrics)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\n",
      "    output_loss_metrics=model._output_loss_metrics)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 311, in train_on_batch\n",
      "    output_loss_metrics=output_loss_metrics))\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 252, in _process_single_batch\n",
      "    training=training))\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 127, in _model_loss\n",
      "    outs = model(inputs, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\n",
      "    outputs = self.call(cast_inputs, *args, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 708, in call\n",
      "    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 860, in _run_internal_graph\n",
      "    output_tensors = layer(computed_tensors, **kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 211, in __call__\n",
      "    return super(AlbertModel, self).__call__(inputs, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\n",
      "    outputs = self.call(cast_inputs, *args, **kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 242, in call\n",
      "    sequence_output = self.encoder(embedding_tensor, attention_mask)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 889, in __call__\n",
      "    return super(Transformer, self).__call__(inputs=inputs, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\n",
      "    outputs = self.call(cast_inputs, *args, **kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 908, in call\n",
      "    output_tensor = self.shared_layer(output_tensor, attention_mask,**kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 805, in __call__\n",
      "    return super(TransformerBlock, self).__call__(inputs, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\n",
      "    outputs = self.call(cast_inputs, *args, **kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 813, in call\n",
      "    attention_mask=attention_mask,**kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/kaggle-nqa/scripts/albert.py\", line 458, in __call__\n",
      "    return super(Attention, self).__call__(inputs, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 753, in __call__\n",
      "    input_masks = self._collect_input_masks(inputs, args, kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1991, in _collect_input_masks\n",
      "    inputs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 536, in map_structure\n",
      "    expand_composites=expand_composites)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 461, in pack_sequence_as\n",
      "    0, is_seq)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 405, in _packed_nest_with_indices\n",
      "    for s in _yield_value(structure):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 165, in _yield_value\n",
      "    for _, v in _yield_sorted_items(iterable):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 195, in _yield_sorted_items\n",
      "    elif _is_namedtuple(iterable):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 93, in _is_namedtuple\n",
      "    def _is_namedtuple(instance, strict=False):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/ejmejm/anaconda3/envs/tf2/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create training callbacks\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(FLAGS.output_dir, FLAGS.output_checkpoint_file), monitor='val_acc', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='max', period=1)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=FLAGS.log_dir, update_freq=128)\n",
    "\n",
    "# Change this to True to actually train\n",
    "if True:\n",
    "    H = model.fit_generator(data_generator({'batch_size': FLAGS.train_batch_size}),\n",
    "                            steps_per_epoch=FLAGS.train_num_precomputed // FLAGS.train_batch_size,\n",
    "                            epochs=FLAGS.num_train_epochs,\n",
    "                            callbacks=[ckpt_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download for Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Unzip and run using \"tensorboard --logdir=\"./logs\" --port=6006\"\n",
    "tar = tarfile.open(\"logs.tar.gz\", \"w:gz\")\n",
    "tar.add(FLAGS.log_dir, arcname=\"logs\")\n",
    "tar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
